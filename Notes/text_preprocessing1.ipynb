{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe5aLuoAiMUR",
        "colab_type": "text"
      },
      "source": [
        "# How to Clean Text for Machine Learning with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVUi1ADFidUJ",
        "colab_type": "text"
      },
      "source": [
        "Source: https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
        "\n",
        "by Jason Brownlee on October 18, 2017 in Deep Learning for Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl7w5Ckkilfd",
        "colab_type": "text"
      },
      "source": [
        "Text preparation methods are specific on the natural language processing task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90rTleuRoRUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "bb0b32d7-8915-4190-e580-f19955590d52"
      },
      "source": [
        "!wget http://www.gutenberg.org/cache/epub/5200/pg5200.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-29 10:32:49--  http://www.gutenberg.org/cache/epub/5200/pg5200.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 141420 (138K) [text/plain]\n",
            "Saving to: ‘pg5200.txt’\n",
            "\n",
            "\rpg5200.txt            0%[                    ]       0  --.-KB/s               \rpg5200.txt          100%[===================>] 138.11K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-06-29 10:32:49 (1.01 MB/s) - ‘pg5200.txt’ saved [141420/141420]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOqUhdzworuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv pg5200.txt metamorphosis.txt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRXK_zmgpDzv",
        "colab_type": "text"
      },
      "source": [
        "## Text Cleaning is Task Specific"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKVOqQtEpoSu",
        "colab_type": "text"
      },
      "source": [
        "- It’s plain text so there is no markup to parse (yay!).\n",
        "- The translation of the original German uses UK English (e.g. “travelling“).\n",
        "- The lines are artificially wrapped with new lines at about 70 characters (meh).\n",
        "- There are no obvious typos or spelling mistakes.\n",
        "- There’s punctuation like commas, apostrophes, quotes, question marks, and more.\n",
        "- There’s hyphenated descriptions like “armour-like”.\n",
        "- There’s a lot of use of the em dash (“-“) to continue sentences (maybe replace with commas?).\n",
        "- There are names (e.g. “Mr. Samsa“)\n",
        "There does not appear to be numbers that require handling (e.g. 1999)\n",
        "- There are section markers (e.g. “II” and “III”), and we have removed the first “I”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu5TQLbBp3Pt",
        "colab_type": "text"
      },
      "source": [
        "## Manual Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqqhdnIwqn_H",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkO41aGKquw4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b844396b-6f3f-4e14-acd3-bea4a0a44e2c"
      },
      "source": [
        "filename = 'metamorphosis.txt'\n",
        "with open(filename, 'r') as file:\n",
        "    text = file.read()\n",
        "    print(text[:70])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "him\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOunrowZr1Qn",
        "colab_type": "text"
      },
      "source": [
        "### Split by Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBu4Lq5pq78y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "31037e2a-b681-461f-8c23-103a99d01b6f"
      },
      "source": [
        "# load text\n",
        "filename = 'metamorphosis.txt'\n",
        "with open(filename, 'rt') as file:\n",
        "    text = file.read()\n",
        "words = text.split(' ')\n",
        "print(words[:20])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found\\nhimself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbZ8m0QJrQgy",
        "colab_type": "text"
      },
      "source": [
        "### Select Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B8EDRnrr6lM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "64256a5d-bc4d-4863-fd1c-eb4a798957ee"
      },
      "source": [
        "import re\n",
        "filename = 'metamorphosis.txt'\n",
        "with open(filename, 'rt') as file:\n",
        "    text = file.read()\n",
        "words = re.split(r'\\W+', text)\n",
        "print(words[:20])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdr1KVtZsKoe",
        "colab_type": "text"
      },
      "source": [
        "### Split by Whitespace and Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWrn_0mhsrdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "916e14fe-d698-4348-d8a2-a07739c48b5f"
      },
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB_UVN4usvtn",
        "colab_type": "text"
      },
      "source": [
        "We can use the function maketrans() to create a mapping table. We can create an empty mapping table, but the third argument of this function allows us to list all of the characters to remove during the translation process. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uenZiSS8s1SH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = str.maketrans('', '', string.punctuation)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9LaKpsRs9eQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f81ddc6f-b7a8-4498-c60c-28f832d025a8"
      },
      "source": [
        "# load text\n",
        "filename = 'metamorphosis.txt'\n",
        "with open(filename, 'rt') as file:\n",
        "    text = file.read()\n",
        "# split into words with white space\n",
        "words = text.split()\n",
        "# remove punctuation\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "stripped = [word.translate(table) for word in words]\n",
        "print(stripped[:20])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6irpCDhtjV2",
        "colab_type": "text"
      },
      "source": [
        "### Normalizing Case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAKMaOpptpxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "99e4516d-2f22-43e6-8b19-b5108a526db0"
      },
      "source": [
        "filename = 'metamorphosis.txt'\n",
        "with open(filename, 'rt') as file:\n",
        "    text = file.read()\n",
        "words = text.split()\n",
        "lowercased = [word.lower() for word in words]\n",
        "print(lowercased[:20])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['one', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geUJzXDgt89Y",
        "colab_type": "text"
      },
      "source": [
        "### Note:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc3iZU0puA5L",
        "colab_type": "text"
      },
      "source": [
        "Remember, simple is better. Start with simple, it not performs as you expect you can make things more complex later to see if it performs better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZeRGSCVuQvT",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vht9F07wuSmO",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization and Cleaning with NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeI8kM_xuU88",
        "colab_type": "text"
      },
      "source": [
        "### Install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENlVg5Mi-EuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2T4vzr0-HPE",
        "colab_type": "text"
      },
      "source": [
        "### Split into Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSUV0jvK-zrR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2cae8136-6cec-41ca-96a4-c9654f831c13"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmvaXmNG-ZKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "695c3e52-f5a0-4e3c-da74-ccd48fd67772"
      },
      "source": [
        "# load text\n",
        "filename = 'metamorphosis.txt'\n",
        "with open(filename, 'rt') as file:\n",
        "    text = file.read()\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "himself transformed in his bed into a horrible vermin.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lw3sQrK-yYA",
        "colab_type": "text"
      },
      "source": [
        "### Split into Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ7Y-nLF_Tke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98a562b5-42c4-4c9d-d6a9-dd5b505753f4"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:5])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['One', 'morning', ',', 'when', 'Gregor']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8jEdSSX_cYP",
        "colab_type": "text"
      },
      "source": [
        "### Filter Out Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMBBEkaO_t0T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5fd4014d-f2ea-41f6-9436-2c1817e05da7"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "words = [word for word in tokens if word.isalpha()]\n",
        "print(words[:20])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd1bSMEI_-eR",
        "colab_type": "text"
      },
      "source": [
        "### Filter out Stop Words (and Pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YHljrw_AZQP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "de6c4b15-e27a-4068-d9b2-3f7023c2722a"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmVyeMJ3AJxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f86387f5-774c-4930-f91c-02d96ed994f9"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlvT2hWjAXbu",
        "colab_type": "text"
      },
      "source": [
        "Text prepration pipeline:\n",
        "- Load raw text\n",
        "- Split into tokens\n",
        "- Convert to lowercase\n",
        "- Remove punctuation from each token\n",
        "- Filter out remaining tokens that are not alphabetic\n",
        "- Filter out tokens that are stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtdBMuQTBEhz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "228c1642-0685-4a15-efc2-bcdc0e6d7c84"
      },
      "source": [
        "# imports\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "# load text\n",
        "filename = 'metamorphosis.txt'\n",
        "with open(filename, 'rt') as file:\n",
        "    text = file.read()\n",
        "# tokenization\n",
        "tokens = word_tokenize(text)\n",
        "# lowercasing\n",
        "tokens = [token.lower() for token in tokens]\n",
        "# remove punctuation\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "tokens = [token.translate(table) for token in tokens]\n",
        "# filter not alphabetic\n",
        "tokens = [token for token in tokens if token.isalpha()]\n",
        "# filter stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "print(tokens[:20])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z5FbDkRCG4G",
        "colab_type": "text"
      },
      "source": [
        "### Stem Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN-8EkDpCPdQ",
        "colab_type": "text"
      },
      "source": [
        "Stemming refers to the process of reducing each word to its root or base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEb4UiSkCVJh",
        "colab_type": "text"
      },
      "source": [
        "Many stemming algorithms, the popular one is Porter Stemming Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2JGp2AgCfVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "963b9a05-1879-4ca1-ac48-266b7935fca7"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "tokens = word_tokenize(text)\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(token) for token in tokens]\n",
        "print(stemmed[:20])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['one', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}