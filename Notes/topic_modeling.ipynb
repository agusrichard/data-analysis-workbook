{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_modeling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTkfmPiWeYNC",
        "colab_type": "text"
      },
      "source": [
        "# NLP with Python: Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_C2GelkhRPs",
        "colab_type": "text"
      },
      "source": [
        "Source: https://sanjayasubedi.com.np/nlp/nlp-with-python-topic-modeling/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ckBlo-zhCQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFRRcMqejJ2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "c44fe57c-da61-464b-8e23-fc6c70e794ba"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN2QPbIlhLf5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "17dfdeaa-e1d4-4966-c496-a7a72b1a4743"
      },
      "source": [
        "# download data\n",
        "!wget http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-01 13:16:43--  http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\n",
            "Resolving mlg.ucd.ie (mlg.ucd.ie)... 137.43.93.132\n",
            "Connecting to mlg.ucd.ie (mlg.ucd.ie)|137.43.93.132|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2874078 (2.7M) [application/zip]\n",
            "Saving to: ‘bbc-fulltext.zip’\n",
            "\n",
            "bbc-fulltext.zip    100%[===================>]   2.74M  3.99MB/s    in 0.7s    \n",
            "\n",
            "2020-07-01 13:16:44 (3.99 MB/s) - ‘bbc-fulltext.zip’ saved [2874078/2874078]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl5IheafhV3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/bbc-fulltext.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/bbc-fulltext')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY82T5OJhXW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0edb1a7d-6fa5-45d5-db2d-974e9738bb51"
      },
      "source": [
        "from sklearn.datasets import load_files\n",
        "path = '/content/bbc-fulltext/bbc'\n",
        "files = load_files(path, encoding='utf-8', decode_error='replace')\n",
        "print(files)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zPJL6arhZHm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "87a7b59f-c075-4bdf-c4be-277f8dca0fc6"
      },
      "source": [
        "df = pd.DataFrame(list(zip(files['data'], files['target'])), columns=['text', 'label'])\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tate &amp; Lyle boss bags top award\\n\\nTate &amp; Lyle...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Halo 2 sells five million copies\\n\\nMicrosoft ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MSPs hear renewed climate warning\\n\\nClimate c...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pavey focuses on indoor success\\n\\nJo Pavey wi...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tories reject rethink on axed MP\\n\\nSacked MP ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  Tate & Lyle boss bags top award\\n\\nTate & Lyle...      0\n",
              "1  Halo 2 sells five million copies\\n\\nMicrosoft ...      4\n",
              "2  MSPs hear renewed climate warning\\n\\nClimate c...      2\n",
              "3  Pavey focuses on indoor success\\n\\nJo Pavey wi...      3\n",
              "4  Tories reject rethink on axed MP\\n\\nSacked MP ...      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIHiqdOvhv-z",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTUOEQhahwx3",
        "colab_type": "text"
      },
      "source": [
        "Topic modeling is an interesting problem in NLP applications where we want to get an idea of what topics we have in our dataset. A topic is nothing more than a collection of words that describe the overall theme. For example, in case of news articles, we might think of topics as politics, sports etc. but topic modeling won’t directly give you names of the topics but rather a set of most probable words that might describe a topic. It is up to us to determine what topic the set of words might refer to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99I_Gf6fhxOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fd52a323-9127-4516-c86a-ad4fcb9b08a0"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "some_text = 'Topic modeling is an interesting problem in NLP applications where we want to get an idea of what topics we have in our dataset.'\n",
        "doc = nlp(some_text)\n",
        "print([(token.lemma_, token.pos_) for token in doc])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('topic', 'ADJ'), ('modeling', 'NOUN'), ('be', 'AUX'), ('an', 'DET'), ('interesting', 'ADJ'), ('problem', 'NOUN'), ('in', 'ADP'), ('NLP', 'PROPN'), ('application', 'NOUN'), ('where', 'ADV'), ('-PRON-', 'PRON'), ('want', 'VERB'), ('to', 'PART'), ('get', 'AUX'), ('an', 'DET'), ('idea', 'NOUN'), ('of', 'ADP'), ('what', 'PRON'), ('topic', 'NOUN'), ('-PRON-', 'PRON'), ('have', 'AUX'), ('in', 'ADP'), ('-PRON-', 'DET'), ('dataset', 'NOUN'), ('.', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03kwb8f-jpaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def only_nouns(text):\n",
        "    doc = nlp(text)\n",
        "    noun_text = ' '.join(token.lemma_ for token in doc if token.pos_ == 'NOUN')\n",
        "    return noun_text"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx13EFDskNfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c716b6d8-a809-4616-c3a6-21e2a28f3c79"
      },
      "source": [
        "df['text'] = df['text'].apply(only_nouns)\n",
        "df['text'].head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    boss bag award executive business magazine tit...\n",
              "1    copy bumper sale fi shooter game copy sale com...\n",
              "2    msp climate warning climate change control dec...\n",
              "3    pavey success view week race track bronze inju...\n",
              "4    tory rethink association candidate election ag...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVLbSTKxkT20",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlkykUVKkoM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ca24291f-a9cf-4806-e864-154b9a1e0588"
      },
      "source": [
        "n_topics = 5\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer(stop_words='english')\n",
        "features = vec.fit_transform(df['text'])\n",
        "\n",
        "from sklearn.decomposition import NMF\n",
        "nmf = NMF(n_components=n_topics)\n",
        "nmf.fit(features)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
              "    n_components=5, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g3ZXuTZlDiv",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjF-1zUYlGNS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "20488166-5ba6-4e02-885c-12b66029952e"
      },
      "source": [
        "feature_names = vec.get_feature_names()\n",
        "for i, topic in enumerate(nmf.components_):\n",
        "    inds = topic.argsort()[-15:]\n",
        "    words = np.array(feature_names)[inds]\n",
        "    print(i, ' '.join(words))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 quarter month analyst oil profit firm price share company market rate year sale economy growth\n",
            "1 ceremony category prize festival role comedy year movie nomination director star actress actor award film\n",
            "2 minute champion title victory goal coach season win time club injury team match player game\n",
            "3 vote taxis country voter issue plan chancellor campaign people leader minister tax government party election\n",
            "4 firm site video device network tv computer software broadband user service technology music people phone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYKLLHUWl1SV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_articles = [\n",
        "    \"Playstation network was down so many people were angry\",\n",
        "    \"Germany scored 7 goals against Brazil in worldcup semi-finals\"\n",
        "]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpYCLkhlntU9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79bb4dc1-6b6d-4499-ba69-256e454a9881"
      },
      "source": [
        "new_features = vec.transform(new_articles)\n",
        "transformed = nmf.transform(new_features)\n",
        "transformed.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxsx8jKYoBaG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24c56c4f-a520-442a-cbe9-e9b887a282d9"
      },
      "source": [
        "transformed.argmax(axis=1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KYceKqaoPu0",
        "colab_type": "text"
      },
      "source": [
        "## Using LatentDirichletAllocation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KaYZ9aOpf4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def only_nouns(text):\n",
        "    doc = nlp(text)\n",
        "    noun_text = ' '.join(token.lemma_ for token in doc if token.pos_ == 'NOUN')\n",
        "    return noun_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbWE7zl-pmqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OnlyNouns(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, texts, y=None):\n",
        "        output = []\n",
        "        for text in texts:\n",
        "            doc = nlp(text)\n",
        "            noun_text = ' '.join(token.lemma_ for token in doc if token.pos_ == 'NOUN')\n",
        "            output.append(noun_text)\n",
        "        return output"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztNpnRzhoSal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_topics = 5\n",
        "pipeline = Pipeline([('only_nouns', OnlyNouns()),\n",
        "                     ('vectorizer', TfidfVectorizer(stop_words='english')),\n",
        "                     ('transformer', LatentDirichletAllocation(n_components=n_topics))])"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMi0NusVowtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "1ef271bd-b721-41a4-cf40-3eaf6a34bf7e"
      },
      "source": [
        "pipeline.fit(df['text'])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('only_nouns', OnlyNouns()),\n",
              "                ('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words='english', strip_accents=No...\n",
              "                                 vocabulary=None)),\n",
              "                ('transformer',\n",
              "                 LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
              "                                           evaluate_every=-1,\n",
              "                                           learning_decay=0.7,\n",
              "                                           learning_method='batch',\n",
              "                                           learning_offset=10.0,\n",
              "                                           max_doc_update_iter=100, max_iter=10,\n",
              "                                           mean_change_tol=0.001,\n",
              "                                           n_components=5, n_jobs=None,\n",
              "                                           perp_tol=0.1, random_state=None,\n",
              "                                           topic_word_prior=None,\n",
              "                                           total_samples=1000000.0,\n",
              "                                           verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PshoC6KDpCgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = pipeline.named_steps['vectorizer'].get_feature_names()\n",
        "components = pipeline.named_steps['transformer'].components_"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "przwGNzFpMM2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ee893155-b112-4e1a-b6fc-c24cc74267f4"
      },
      "source": [
        "for i, topic in enumerate(components):\n",
        "    inds = topic.argsort()[-15:]\n",
        "    words = ' '.join(np.array(feature_names)[inds])\n",
        "    print(f\"Topic {i}: {words}\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 0: hole panda fix program writer patch axe leinster spyware software ballet attachment flaw virus patent\n",
            "Topic 1: star nomination actress year role chart ceremony category band number singer director award actor film\n",
            "Topic 2: seed week minute club year coach title victory injury season time match team player game\n",
            "Topic 3: service month user profit price technology share phone music market company sale year firm people\n",
            "Topic 4: law job policy rate campaign year country issue economy plan tax leader people government election\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOZB0s0iqzTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_articles = [\n",
        "    \"Playstation network was down so many people were angry\",\n",
        "    \"Germany scored 7 goals against Brazil in worldcup semi-finals\"\n",
        "]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn6x3G8NrOJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_features = pipeline.named_steps['vectorizer'].transform(new_articles)\n",
        "transformed = pipeline.named_steps['transformer'].transform(new_features)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbK2AvgQsIEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d02d6b4b-3388-4157-9b6f-371f3489ba63"
      },
      "source": [
        "transformed.argmax(axis=1)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    }
  ]
}