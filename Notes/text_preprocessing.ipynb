{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRR9G_Crase-",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing in Python: Steps, Tools, and Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xierOT24a3xt",
        "colab_type": "text"
      },
      "source": [
        "Source: https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
        "\n",
        "by Olga Davydova, Data Monsters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYk4knI-a64z",
        "colab_type": "text"
      },
      "source": [
        "After we have the data, we start with text normalization. Which includes:\n",
        "- Converting all letters to lower or upper case.\n",
        "- Converting numbers into words or removing numbers.\n",
        "- Removing punctuations, accent marks and other diacritics.\n",
        "- Removing white spaces.\n",
        "- Exapanding abbreviations.\n",
        "- Removing stop words, sparse terms, and particular words.\n",
        "- Text canonicalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEv6sobObk-m",
        "colab_type": "text"
      },
      "source": [
        "## Convert text to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLhw81xObq5-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7dfca5fc-5b12-4ebe-8fdd-9b3f744542af"
      },
      "source": [
        "input_str = 'Hey Jude, dont make it bad. Take a sad song, Jude. Sekar tell me to shut up.'\n",
        "print(input_str.lower())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hey jude, dont make it bad. take a sad song, jude. sekar tell me to shut up.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pPA7K8kb6bX",
        "colab_type": "text"
      },
      "source": [
        "## Removing numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg9PJUzeb_f5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4655e203-fe3d-40d4-9468-27aadca11de0"
      },
      "source": [
        "import re\n",
        "input_str = \"Box A contains 3 red balls and box B contains 4 blue balls\"\n",
        "print(re.sub(r'\\d+', '', input_str))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box A contains  red balls and box B contains  blue balls\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozR9Qg9fc6lm",
        "colab_type": "text"
      },
      "source": [
        "## Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHRRfsR6cSVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb77b4ef-a196-465d-fdb5-3cedf82eb2dc"
      },
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZtBoTk2c5BV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xiTd9JxdgpL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "29f598f8-b5bc-47a1-c5b9-a4bf261bae75"
      },
      "source": [
        "input_str.translate(str.maketrans('', '', string.punctuation))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'This is an example of string with punctuation'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JotgKWhtdsVm",
        "colab_type": "text"
      },
      "source": [
        "## Remove whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1gFQ96QfBni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "84bd8bed-c54c-45a7-84a5-1214594cd253"
      },
      "source": [
        "input_str = \" \\t a string example\\t \"\n",
        "input_str.strip()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'a string example'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwM8_lwGfJqS",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbTfXLeEfMTq",
        "colab_type": "text"
      },
      "source": [
        "Is the process of splitting the given text into smaller pieces called tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_HfIxs506KJ",
        "colab_type": "text"
      },
      "source": [
        "## Remove stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoQ8Om7g089z",
        "colab_type": "text"
      },
      "source": [
        "Are the most commons words in a language like 'the', 'a', 'on', 'is', 'all'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0nz-Vyy1Na8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5f250d02-234f-4e1b-842a-67cb85be1afa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "from nltk.corpus import stopwords\n",
        "print(set(stopwords.words('english')))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "{'ours', 'aren', 'theirs', 'during', 'wasn', 'him', 'because', 'whom', 'too', 'off', \"needn't\", 'such', 'who', 'their', 'those', 'against', \"won't\", 'll', 'our', \"you're\", \"should've\", 'haven', 'most', 'have', \"you'd\", 'on', 'other', 'until', 'these', 'hers', 'yours', 'yourselves', 'so', \"wouldn't\", 'only', \"that'll\", 'be', 'when', 'mustn', 'isn', 'to', 'being', 'few', 'mightn', 'very', 'itself', 'won', 'at', 'had', 're', 'shan', \"weren't\", 'between', 'before', 'she', \"didn't\", 'by', 'some', 'can', 'needn', 'which', 'an', 'a', 'into', 'further', 've', 'if', 'out', 'again', 'but', 'm', 'over', 'were', 'o', 'or', 'below', 'this', 'with', 'no', \"mightn't\", 'then', 'each', \"aren't\", 'her', 'there', 'ourselves', 'any', 'about', 'how', 'his', \"she's\", 'what', 'and', \"shouldn't\", 'myself', 'themselves', 'more', 'you', 'herself', \"shan't\", 'we', 'the', \"isn't\", 'am', \"you'll\", 'from', 'doesn', 'my', 'your', 's', 'above', 'yourself', 'they', 'of', 'as', 'don', 'hadn', \"hadn't\", \"mustn't\", 'up', 'been', \"it's\", 'all', \"you've\", \"doesn't\", 'himself', 'just', 'me', 'while', 'down', \"couldn't\", 'didn', 'weren', 'where', 'in', 'ain', 'will', 'doing', 'once', 'shouldn', 'after', 'its', 'couldn', 'why', 'that', 'for', 'own', \"wasn't\", 'he', 'them', 'i', 'ma', 'was', 'does', 'it', 'hasn', 'nor', 'having', 'now', 'same', 'd', 'than', 'both', 'y', 'wouldn', \"don't\", \"haven't\", 'are', 'not', 'here', 'did', 't', 'is', 'under', 'should', 'has', \"hasn't\", 'do', 'through'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrCMvCRk1aq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a39db743-cfff-4346-c27e-5c954243fa6d"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "tokens = word_tokenize(input_str)\n",
        "result = [token for token in tokens if not token in set(stopwords.words('english'))]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jfYmrsQ2emD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4126c428-1e18-4495-bc91-13b4d630a0df"
      },
      "source": [
        "result"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLTK',\n",
              " 'leading',\n",
              " 'platform',\n",
              " 'building',\n",
              " 'Python',\n",
              " 'programs',\n",
              " 'work',\n",
              " 'human',\n",
              " 'language',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb322r1j2qDJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "408f1f1d-68f3-444c-e6a3-c1b83751bf3e"
      },
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "result = [token for token in tokens if token not in ENGLISH_STOP_WORDS]\n",
        "print(result)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1yga-wF2zEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f89c7283-7b99-48eb-df38-9c6a2bb18ec1"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "result = [token for token in tokens if token not in STOP_WORDS]\n",
        "print(result)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UITGKaLy3Gpz",
        "colab_type": "text"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFFee2CY3YHT",
        "colab_type": "text"
      },
      "source": [
        "Is a process of reducing words to their word stem, base or root forms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEP6WJME3c8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "dc275ceb-5dd6-4164-bdbc-318e0e8fb701"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "input_str = \"There are several types of stemming algorithms.\"\n",
        "stemmer = PorterStemmer()\n",
        "input_str = word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(stemmer.stem(word))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there\n",
            "are\n",
            "sever\n",
            "type\n",
            "of\n",
            "stem\n",
            "algorithm\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeZXSKZd31aU",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk6RDs8K36r3",
        "colab_type": "text"
      },
      "source": [
        "As opposed to Stemming, Lemmatization does not simple chop off inflections, instead it uses lexical knowledge bases to get the correct base forms of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AxCaKTW49lb",
        "colab_type": "text"
      },
      "source": [
        "Lemmatization tools are presented libraries described above: NLTK (WordNet Lemmatizer), spaCy, TextBlob, Pattern, gensim, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), Illinois Lemmatizer, and DKPro Core."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfhX8lPy4MuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b97b4b80-3a67-4b53-f49f-7069eea52648"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "input_str = \"been had done languages cities mice\"\n",
        "input_str = word_tokenize(input_str)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for word in input_str:\n",
        "    print(lemmatizer.lemmatize(word))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "been\n",
            "had\n",
            "done\n",
            "language\n",
            "city\n",
            "mouse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGA6wxEt4rSE",
        "colab_type": "text"
      },
      "source": [
        "## Part of Speech tagging (POS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQTcvi-B5KzX",
        "colab_type": "text"
      },
      "source": [
        "Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. There are many tools containing POS taggers including NLTK, spaCy, TextBlob, Pattern, Stanford CoreNLP, Memory-Based Shallow Parser (MBSP), Apache OpenNLP, Apache Lucene, General Architecture for Text Engineering (GATE), FreeLing, Illinois Part of Speech Tagger, and DKPro Core."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4fbXW8G5Lr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_str = \"Parts of speech examples: an article, to write, interesting, easily, and, of\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt81etbc5Oea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b3e7ba2-531b-4161-a98b-da4c47bd9cfb"
      },
      "source": [
        "from textblob import TextBlob\n",
        "result = TextBlob(input_str)\n",
        "print(result)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parts of speech examples: an article, to write, interesting, easily, and, of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfJRjVrk5c3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "bcc61b7f-8157-4afb-ee42-054a42d6e8ca"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(result.tags)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvJjCE9x5hTp",
        "colab_type": "text"
      },
      "source": [
        "## Chunking (Shallow Parsing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrTx_3GM50AF",
        "colab_type": "text"
      },
      "source": [
        "Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.) [23]. Chunking tools: NLTK, TreeTagger chunker, Apache OpenNLP, General Architecture for Text Engineering (GATE), FreeLing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT0rp-BJ55iV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_str = \"A black television and a white stove were bought for the new apartment of John.\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-_AigPR7scx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6c1a0e20-28b7-48a1-f6bf-3427fcb18910"
      },
      "source": [
        "result = TextBlob(input_str)\n",
        "print(result.tags)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('A', 'DT'), ('black', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('a', 'DT'), ('white', 'JJ'), ('stove', 'NN'), ('were', 'VBD'), ('bought', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('apartment', 'NN'), ('of', 'IN'), ('John', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtErqYWF7wBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "07aef512-52cc-4781-c033-67a6878b6db1"
      },
      "source": [
        "reg_exp = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "rp = nltk.RegexpParser(reg_exp)\n",
        "result = rp.parse(result.tags)\n",
        "print(result)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP A/DT black/JJ television/NN)\n",
            "  and/CC\n",
            "  (NP a/DT white/JJ stove/NN)\n",
            "  were/VBD\n",
            "  bought/VBN\n",
            "  for/IN\n",
            "  (NP the/DT new/JJ apartment/NN)\n",
            "  of/IN\n",
            "  John/NNP)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_0yjcoX8GGx",
        "colab_type": "text"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY4EGPQsXZTo",
        "colab_type": "text"
      },
      "source": [
        "Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFmetAJoXbwp",
        "colab_type": "text"
      },
      "source": [
        "Named-entity recognition tools: NLTK, spaCy, General Architecture for Text Engineering (GATE) — ANNIE, Apache OpenNLP, Stanford CoreNLP, DKPro Core, MITIE, Watson Natural Language Understanding, TextRazor, FreeLing are described in the “NER” sheet of the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gObBzQeXdpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5a2d7561-9e20-42ce-a875-476123f41ebd"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "input_str = \"Bill works for Apple so he went to Boston for a conference.\"\n",
        "print(word_tokenize(input_str))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Bill', 'works', 'for', 'Apple', 'so', 'he', 'went', 'to', 'Boston', 'for', 'a', 'conference', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVXGWCPYXt5y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b71dc2e1-5fa6-438e-9c5e-c467d76ed31a"
      },
      "source": [
        "pos_tag(word_tokenize(input_str))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Bill', 'NNP'),\n",
              " ('works', 'VBZ'),\n",
              " ('for', 'IN'),\n",
              " ('Apple', 'NNP'),\n",
              " ('so', 'IN'),\n",
              " ('he', 'PRP'),\n",
              " ('went', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('Boston', 'NNP'),\n",
              " ('for', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('conference', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VM8YxSlXxQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "83ba0334-4698-4b97-8472-d127348be151"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "print(ne_chunk(pos_tag(word_tokenize(input_str))))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  (PERSON Bill/NNP)\n",
            "  works/VBZ\n",
            "  for/IN\n",
            "  Apple/NNP\n",
            "  so/IN\n",
            "  he/PRP\n",
            "  went/VBD\n",
            "  to/TO\n",
            "  (GPE Boston/NNP)\n",
            "  for/IN\n",
            "  a/DT\n",
            "  conference/NN\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmIGkvwvYCbz",
        "colab_type": "text"
      },
      "source": [
        "## Coreference Resolution (Anaphora Resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb8M2sT8ZbPC",
        "colab_type": "text"
      },
      "source": [
        "Pronouns and other referring expressions should be connected to the right individuals. Coreference resolution finds the mentions in a text that refer to the same real-world entity. For example, in the sentence, “Andrew said he would buy a car” the pronoun “he” refers to the same person, namely to “Andrew”. Coreference resolution tools: Stanford CoreNLP, spaCy, Open Calais, Apache OpenNLP are described in the “Coreference resolution” sheet of the table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du6AP27mZxTX",
        "colab_type": "text"
      },
      "source": [
        "## Collocation Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWKKf1D_ZzKv",
        "colab_type": "text"
      },
      "source": [
        "Collocations are word combinations occurring together more often than would be expected by chance. Collocation examples are “break the rules,” “free time,” “draw a conclusion,” “keep in mind,” “get ready,” and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IApYyvk-a5po",
        "colab_type": "text"
      },
      "source": [
        "## Relationship Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg34iYsFa7T7",
        "colab_type": "text"
      },
      "source": [
        "Relationship extraction allows obtaining structured information from unstructured sources such as raw text. Strictly stated, it is identifying relations (e.g., acquisition, spouse, employment) among named entities (e.g., people, organizations, locations). For example, from the sentence “Mark and Emily married yesterday,” we can extract the information that Mark is Emily’s husband."
      ]
    }
  ]
}