{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9TMYp27vMUy",
        "colab_type": "text"
      },
      "source": [
        "# Text classification from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSgUtSZtwM8x",
        "colab_type": "text"
      },
      "source": [
        "__Authors__: Mark Omernick, Francois Chollet\n",
        "\n",
        "__Date created__: 2019/11/06\n",
        "\n",
        "__Last modified__: 2020/05/17\n",
        "\n",
        "__Description__: Text sentiment classification starting from raw text files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sPXNZ298Fi0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "f5a8b56a-387d-4d7b-fb06-5a581ab280cc"
      },
      "source": [
        "!pip install tf-nightly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.4.0.dev20200711)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: tb-nightly<2.4.0a0,>=2.3.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.3.0a20200711)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied: tf-estimator-nightly in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0.dev2020071101)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.30.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (47.3.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0.post3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.1.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIvoXDFzwOiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cz-AEXuwc9L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "34354947-3eee-4940-e0b2-28a4499a5ac3"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  15.1M      0  0:00:05  0:00:05 --:--:-- 17.1M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnpIgdjkwnRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8dae81f-08ec-4e58-a7c4-6b2c63ce0452"
      },
      "source": [
        "!ls aclImdb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bMPKz-8x33R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26d38c27-91da-410f-976c-b586927b357e"
      },
      "source": [
        "!ls aclImdb/test"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdXAtRASx6Sk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b48b693d-bd18-49ca-a818-a238aa7225b5"
      },
      "source": [
        "!ls aclImdb/train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZsT5BKex_5z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1e880ad0-58e1-40ad-9e8b-3638e0962b74"
      },
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAsjDuNnyJTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_files\n",
        "\n",
        "bunch_train = load_files('/content/aclImdb/train', categories=['pos', 'neg'])\n",
        "bunch_test = load_files('/content/aclImdb/test', categories=['pos', 'neg'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1QpzYdDzQog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "26288132-1245-407c-c784-49c9a26565a2"
      },
      "source": [
        "data = np.array([x.decode() for x in bunch_train.data])\n",
        "train = pd.DataFrame(np.c_[data, bunch_train.target], columns=['review', 'label'])\n",
        "train['label'] = train['label'].astype(int)\n",
        "train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Zero Day leads you to think, even re-think why...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Words can't describe how bad this movie is. I ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Everyone plays their part pretty well in this ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>There are a lot of highly talented filmmakers/...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I've just had the evidence that confirmed my s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  label\n",
              "0  Zero Day leads you to think, even re-think why...      1\n",
              "1  Words can't describe how bad this movie is. I ...      0\n",
              "2  Everyone plays their part pretty well in this ...      1\n",
              "3  There are a lot of highly talented filmmakers/...      0\n",
              "4  I've just had the evidence that confirmed my s...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l46V2fr4zxkK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bfc7eafa-0d39-400a-e10e-c71c6b4a7594"
      },
      "source": [
        "data = np.array([x.decode() for x in bunch_test.data])\n",
        "test = pd.DataFrame(np.c_[data, bunch_test.target], columns=['review', 'label'])\n",
        "test['label'] = test['label'].astype(int)\n",
        "test.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Don't hate Heather Graham because she's beauti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I don't know how this movie has received so ma...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I caught this movie on the Horror Channel and ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NBC had a chance to make a powerful religious ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Looking for something shocking? Okay fine... t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  label\n",
              "0  Don't hate Heather Graham because she's beauti...      1\n",
              "1  I don't know how this movie has received so ma...      0\n",
              "2  I caught this movie on the Horror Channel and ...      1\n",
              "3  NBC had a chance to make a powerful religious ...      0\n",
              "4  Looking for something shocking? Okay fine... t...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6ZsuCcM4UcI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "19434eb3-ac04-4b76-a78e-7f34784c45ec"
      },
      "source": [
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"/content/aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Number of batches in raw_train_ds: %d\"\n",
        "    % tf.data.experimental.cardinality(raw_train_ds)\n",
        ")\n",
        "print(\n",
        "    \"Number of batches in raw_val_ds: %d\" % tf.data.experimental.cardinality(raw_val_ds)\n",
        ")\n",
        "print(\n",
        "    \"Number of batches in raw_test_ds: %d\"\n",
        "    % tf.data.experimental.cardinality(raw_test_ds)\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 75000 files belonging to 3 classes.\n",
            "Using 60000 files for training.\n",
            "Found 75000 files belonging to 3 classes.\n",
            "Using 15000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Number of batches in raw_train_ds: 1875\n",
            "Number of batches in raw_val_ds: 469\n",
            "Number of batches in raw_test_ds: 782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcnmv3Q35FYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "dd45fd4f-23be-4330-cd78-a74e1009856e"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Intriguing premise completely wasted somehow by Carpenter and crew. Sorry guys, I generally love Carpenter flicks, but this one is horrid garbage. Nearly every possible suspenseful sequence is butchered. Not much to say outside of the synopsis, but imagine Carpenter at his worst, and it only begins to describe the debacle that is \"Prince of Darkness\". I spent weeks searching my local video stores with a friend to find a copy, only to be more disappointed than I have by any of his other films. I love \"The Thing\", \"They Live\", ... heck, even have a soft spot for his Village of the Damned flick. But this.... Well, Thank god for Victor Wong tho. 4/10'\n",
            "2\n",
            "b'This is your only spoiler warning. What a sad state of our cinema when unprofessional junk like this is considered \"Oscar worthy\".<br /><br />I divide material into three levels. The first is the stage theatre. Here the viewer is stagnant and the power rests in the presentation of the actor and, most importantly, in the power of the writer. A good playwright is better than a good screenwriter because he or she knows the ways of words better. The best playwrights know how to create imagery that the barren stage cannot show.<br /><br />The second level is film. In this medium, a weaker writer can be used, but the viewer is not sitting in one spot the entire time. With film, the context can take the qualities of visual poetry and meaning in addition to strong writing. Furthermore, film can manipulate everday elements like sound and color in ways that are almost surreal.<br /><br />The final level is literature. In this context, everything is imagined by the author, translated onto paper, and then re-imagined by the reader. Far more detail can go into a novel than is conceivable for a film studio.<br /><br />This is why adaptations can go up, but never down. Novelizations are never better than the base film (see the dime-per-dozen ones at your local book store), whereas the film cannot convey the same power as the original book (Catch-22 and LotR). Movies can rarely be made into plays and plays can always be made into movies.<br /><br />As for \\'The Last Picture Show\\', it fails. It is a film that should stick to the stage because the director is too stupid to shoot anything right. The characters talk the same and act the same, it\\'s pure futility. Add to that an obnoxious soundtrack and you have an entirely unwatchable film.<br /><br />I saw this in my high school drama class with about 20 other wannabe thespians. The instructor raved about how sad the movie was. What is sad is how such stagnant work is considered depressing when the material itself is hilarious. Had this been in color the scenes of impotence, the pool party, and the old hooker would be considered great comedy. Look what Lucas did with \\'American Graffiti\\' a few years later.<br /><br />The American secondary education system needs to start teaching ABOUT film rather than trying to teach WITH film. Two visually powerful downbeat films: Apocalypse Now and Barry Lyndon. Rely on them, not this. It\\'s the \\'Last\\' I want to see of it. 1 out of 5.'\n",
            "0\n",
            "b\"Why... that's my only question, why this horrible soundtrack of a garage band with no talent , this cast ( i'm sure in some scenes reading off cards ).... why all the clichees ever seen are all in this flick ALL of them, can you one second fall for any of these caracters or actually believe in this cheezy romance. Plain horrible .. stay very very far from this movie please I beg you. There's so much excellent indie film out there. why\"\n",
            "2\n",
            "b\"this film was far from amazing, D D Lewis was very good and Leo De-Caprio was ok but that's where the praise stops. Diaz was terrible, her character had no layers and was deviod of any emotion. The scripting was inconsistent and the direction was sluggish and jaded. There was hardly any plot and the one that was there was full of holes. The humor some people pointed out distracted from the film rather than added to it, it was needless for a film of this nature, Gangs Of New York is meant to be about rivalry, revenge and drama, and although there were a couple of places where the dark humor really worked there were too many instances where the humor felt like tacted-on Hollywood cheese. If you want to get some cheep thrills by seeing a bit of violence and mild nudity then this is the film for you, if you are looking for a cinematic experience don't bother, it'd be a waste of a 3 hours of your life.\"\n",
            "2\n",
            "b\"This is probably the best performance you'll see by Warren William--a very popular leading man in the early to mid 1930s but who has been all but forgotten today. And, since he's no longer a household name, it's not surprising that this film is very much unknown.<br /><br />This story is based on a real individual named Ivar Kreuger, who tried to corner the world market on matches by being about the most unscrupulous and power-hungry man of his day. The film concerns the many ways he shows that down deep he has no soul and there is no sin beyond him if it gains him more money and power.<br /><br />William is exceptional in the role and the film is fascinating from start to finish. Considering the film is about matches, it must have taken considerable writing, acting and directing talents to produce such a captivating film.\"\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kADbOONb9FRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Having looked at our data above, we see that the raw text contains HTML break\n",
        "# tags of the form '<br />'. These tags will not be removed by the default\n",
        "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
        "# create a custom standardization function.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Model constants.\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "# Now that we have our custom standardization, we can instantiate our text\n",
        "# vectorization layer. We are using this layer to normalize, split, and map\n",
        "# strings to integers, so we set our 'output_mode' to 'int'.\n",
        "# Note that we're using the default split function,\n",
        "# and the custom standardization defined above.\n",
        "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
        "# model won't support ragged sequences.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# Now that the vocab layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6CEE41-ipJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1j33L3k_CGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suGlm3xH_EUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "4f9653e6-605e-4cd1-dcb0-89410ade0b06"
      },
      "source": [
        "epochs = 3\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 377s 201ms/step - loss: -377782075392.0000 - accuracy: 0.1664 - val_loss: -2021618876416.0000 - val_accuracy: 0.1677\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 349s 186ms/step - loss: -14578190647296.0000 - accuracy: 0.1664 - val_loss: -36709588795392.0000 - val_accuracy: 0.1677\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 348s 186ms/step - loss: -97876801748992.0000 - accuracy: 0.1664 - val_loss: -178046443192320.0000 - val_accuracy: 0.1677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3c756aa860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huZXyWuy_GBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2b39090e-924e-4a77-cd11-78c14343f192"
      },
      "source": [
        "model.evaluate(test_ds)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 37s 47ms/step - loss: 180112372793344.0000 - accuracy: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[180112372793344.0, 0.5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1ge-wTd_H59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7bad4059-b416-4487-84ca-9323e6f3eb7a"
      },
      "source": [
        "# A string input\n",
        "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
        "# Turn strings into vocab indices\n",
        "indices = vectorize_layer(inputs)\n",
        "# Turn vocab indices into predictions\n",
        "outputs = model(indices)\n",
        "\n",
        "# Our end to end model\n",
        "end_to_end_model = tf.keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 37s 48ms/step - loss: 180112339238912.0000 - accuracy: 0.5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[180112339238912.0, 0.5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}