{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vqK24uBvPs3",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing with Python: Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SziIaySXokg_",
        "colab_type": "text"
      },
      "source": [
        "Source: https://sanjayasubedi.com.np/nlp/nlp-intro/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PWa3sF4tEEP",
        "colab_type": "text"
      },
      "source": [
        "__Import Libraries__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNm3zzgCtAy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import regex as re\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSMFOqsJtce_",
        "colab_type": "text"
      },
      "source": [
        "__NLTK Downloads__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuA-lylcth7_",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4bnkuGGtoSA",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o6z33zqtqEs",
        "colab_type": "text"
      },
      "source": [
        "NLP Flow:\n",
        "Text --> Preprocess --> Feature Extraction --> Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbSOSFw2t2Y8",
        "colab_type": "text"
      },
      "source": [
        "## Preprocesing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEWg2PZtuIrS",
        "colab_type": "text"
      },
      "source": [
        "In this section, I’ll introduce some of the common pre-processing steps. As an input, we have a text. It could be a news article, search query, instructions for a chat-bot etc. We feed this input to a Pre-processing step where we need to extract the tokens, which could be a word or a phrase or even a sentence, and clean our input text i.e. fix spelling mistakes, remove useless words (stop-words), augment the words with part of speech or something else etc. What we do in this step depends on the problem we are trying to solve but for many applications tokenization, stop-word removal and stemming are fairly common"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiVk19qnuJMD",
        "colab_type": "text"
      },
      "source": [
        "__Example Input__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKz80qmuU5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"This warning shouldn't be taken lightly.\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaAmk8fnuY03",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5q9lVEXufmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35bda701-7415-4554-a7db-786df35a8510"
      },
      "source": [
        "print(text.split(' '))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'warning', \"shouldn't\", 'be', 'taken', 'lightly.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaYDIHrluq0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "493b4932-2044-42ac-9e63-f014d7d61b5a"
      },
      "source": [
        "clean_text = re.sub('\\p{P}+', '', text)\n",
        "print(clean_text)\n",
        "print(clean_text.split(' '))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This warning shouldnt be taken lightly\n",
            "['This', 'warning', 'shouldnt', 'be', 'taken', 'lightly']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZW6hb9IvIFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "898b0342-2280-4c17-9dc2-18a62dd7c227"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmpNnYPowMiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6ec23b1-0b3e-46f7-e5ce-68d10f912c55"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "print(doc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This warning shouldn't be taken lightly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnzJbBDMwbL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f825deab-44c4-46b1-c710-1f2976015234"
      },
      "source": [
        "type(doc)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o216ga_qwdxE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2e64d97-d345-480a-c183-fc75daa6036d"
      },
      "source": [
        "print([token for token in doc])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[This, warning, should, n't, be, taken, lightly, .]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ox0YDKwlda",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c3b3781-28e4-4e8a-f6f7-91348176b2df"
      },
      "source": [
        "print([token.text for token in doc])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'warning', 'should', \"n't\", 'be', 'taken', 'lightly', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9VqNY-EwqkI",
        "colab_type": "text"
      },
      "source": [
        "Now the tokenization looks much better. The punctuations are still present but we can easily remove them. Every token produced by spaCy is of type spacy.tokens.token.Token and it has a number of properties. Among them there are a few that start with is_* e.g. is_digit, is_punct, is_stop etc. that can be used to determine what kind of token it is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZxfHmzAw7dy",
        "colab_type": "text"
      },
      "source": [
        "### Stopword Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDd1SU1xw9VK",
        "colab_type": "text"
      },
      "source": [
        "Stop-words are words that occur frequently but don’t carry any meaning on their own. For example, a, an, the occur very frequently and can be discarded without any loss of meaning for most of NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKTin0tRxDaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a0d371bd-7236-432b-c9f6-b57294ddf338"
      },
      "source": [
        "print([(token.text, token.is_stop) for token in doc])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('This', True), ('warning', False), ('should', True), (\"n't\", True), ('be', True), ('taken', False), ('lightly', False), ('.', False)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lkwUCw0xOn2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45cf3f77-d4d5-46ff-d020-4404289877db"
      },
      "source": [
        "print([token.text for token in doc if not token.is_stop])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['warning', 'taken', 'lightly', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLva2TpvxXlG",
        "colab_type": "text"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29UhPRY3xu93",
        "colab_type": "text"
      },
      "source": [
        "Stemming is a process of reducing the words to their root form. For example, stem of cats would be cat, transportation would be transport etc. Again, this is to reduce the size of vocabulary because for most of the applications, distinction between cats and cat is not important. For example, when a user searches for documents containing the word cats but we only have documents containing the word cat, then the user would get zero results. But if we stem the user’s query then we would be able to retrieve some results. A popular algorithm used for stemming is Porter algorithm. spaCy does not have any feature for stemming but libraries like NLTK have such feature. Stemming algorithms are mostly based on rules and the output is not always a valid word. Consider the following examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyevw0hrx5pA",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAi4LtGayDKI",
        "colab_type": "text"
      },
      "source": [
        "Lemmatisation is a more complex version of stemming. Part of speech (POS) of each word is determined and then different rules are applied for different POS. spaCy provides lemmatisation since it is much better than stemming but it is a bit more computationally expensive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6SM4skjyKWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "85ca54cf-e57e-49c8-9dfd-19e4ed7cf0d9"
      },
      "source": [
        "print([(token.text, token.lemma_) for token in nlp('we are meeting tomorrow')])\n",
        "print([(token.text, token.lemma_) for token in nlp('i am going to a meeting')])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('we', '-PRON-'), ('are', 'be'), ('meeting', 'meet'), ('tomorrow', 'tomorrow')]\n",
            "[('i', 'i'), ('am', 'be'), ('going', 'go'), ('to', 'to'), ('a', 'a'), ('meeting', 'meeting')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWC1Mt-rydZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbf6d3a6-ffab-417b-a220-227fe6d5377e"
      },
      "source": [
        "print([token.lemma_ for token in doc])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'warning', 'should', 'not', 'be', 'take', 'lightly', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIxydKTXy4cT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "acc36b79-88c1-449f-b808-e89e5bf686b4"
      },
      "source": [
        "print([(token.text, token.pos_) for token in doc])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('This', 'DET'), ('warning', 'NOUN'), ('should', 'VERB'), (\"n't\", 'PART'), ('be', 'AUX'), ('taken', 'VERB'), ('lightly', 'ADV'), ('.', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1GVv12qzI3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9ff40741-b1e1-45fd-e1c2-267137b27c9d"
      },
      "source": [
        "lemmatized = [token.lemma_ for token in doc]\n",
        "print([(token.text, token.pos_) for token in nlp(' '.join(lemmatized))])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('this', 'DET'), ('warning', 'NOUN'), ('should', 'VERB'), ('not', 'PART'), ('be', 'AUX'), ('take', 'VERB'), ('lightly', 'ADV'), ('.', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}