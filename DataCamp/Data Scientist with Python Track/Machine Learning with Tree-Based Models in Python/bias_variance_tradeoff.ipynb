{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bias_variance_tradeoff.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3PU08km8mQH",
        "colab_type": "text"
      },
      "source": [
        "# The Bias-Variance Tradeoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGAYsITo9FN8",
        "colab_type": "text"
      },
      "source": [
        "## Complexity, bias and variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y41JUTbOAsa3",
        "colab_type": "text"
      },
      "source": [
        "D. As the complexity of f^ increases, the bias term decreases while the variance term increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50obFDSIA5Fi",
        "colab_type": "text"
      },
      "source": [
        "## Overfitting and underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOPpAXadA7Dl",
        "colab_type": "text"
      },
      "source": [
        "C. B suffers from high bias and underfits the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtfhmtD7BILT",
        "colab_type": "text"
      },
      "source": [
        "## Instantiate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZIxZ7ixEq7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import train_test_split from sklearn.model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set SEED for reproducibility\n",
        "SEED = 1\n",
        "\n",
        "# Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
        "\n",
        "# Instantiate a DecisionTreeRegressor dt\n",
        "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNfybizaFBvB",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the 10-fold CV error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPxMdmqWFCIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the array containing the 10-folds CV MSEs\n",
        "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
        "                       scoring='neg_mean_squared_error',\n",
        "                       n_jobs=-1)\n",
        "\n",
        "# Compute the 10-folds CV RMSE\n",
        "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
        "\n",
        "# Print RMSE_CV\n",
        "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gp6bB3lFqvo",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the training error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73QpGduyFrIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import mean_squared_error from sklearn.metrics as MSE\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "\n",
        "# Fit dt to the training set\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the training set\n",
        "y_pred_train = dt.predict(X_train)\n",
        "\n",
        "# Evaluate the training set RMSE of dt\n",
        "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
        "\n",
        "# Print RMSE_train\n",
        "print('Train RMSE: {:.2f}'.format(RMSE_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou4k3CKuGOSm",
        "colab_type": "text"
      },
      "source": [
        "## High bias or high variance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMA4J2yKGOtz",
        "colab_type": "text"
      },
      "source": [
        "B. dt suffers from high bias because RMSE_CV â‰ˆ RMSE_train and both scores are greater than baseline_RMSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQXlrWhbGqGp",
        "colab_type": "text"
      },
      "source": [
        "## Define the ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UmohD5lHoNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set seed for reproducibility\n",
        "SEED=1\n",
        "\n",
        "# Instantiate lr\n",
        "lr = LogisticRegression(random_state=SEED)\n",
        "\n",
        "# Instantiate knn\n",
        "knn = KNN(n_neighbors=27)\n",
        "\n",
        "# Instantiate dt\n",
        "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
        "\n",
        "# Define the list classifiers\n",
        "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DheV0R_3H9Lb",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate individual classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6JO1mxdH9lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterate over the pre-defined list of classifiers\n",
        "for clf_name, clf in classifiers:    \n",
        " \n",
        "    # Fit clf to the training set\n",
        "    clf.fit(X_train, y_train)    \n",
        "   \n",
        "    # Predict y_pred\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred) \n",
        "   \n",
        "    # Evaluate clf's accuracy on the test set\n",
        "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hdgbuDTIQEO",
        "colab_type": "text"
      },
      "source": [
        "## Better performance with a Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPv04z9IIP9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import VotingClassifier from sklearn.ensemble\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Instantiate a VotingClassifier vc\n",
        "vc = VotingClassifier(estimators=classifiers)     \n",
        "\n",
        "# Fit vc to the training set\n",
        "vc.fit(X_train, y_train)   \n",
        "\n",
        "# Evaluate the test set predictions\n",
        "y_pred = vc.predict(X_test)\n",
        "\n",
        "# Calculate accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Voting Classifier: {:.3f}'.format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}