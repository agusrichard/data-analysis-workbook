{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decorelation_and_dimension_reduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkgd13f1QcV1",
        "colab_type": "text"
      },
      "source": [
        "# Decorrelating your data and dimension reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhp7q8r9ReY3",
        "colab_type": "text"
      },
      "source": [
        "## Correlated data in nature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GJv3xn4RufX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assign the 0th column of grains: width\n",
        "width = grains[:, 0]\n",
        "\n",
        "# Assign the 1st column of grains: length\n",
        "length = grains[:, 1]\n",
        "\n",
        "# Scatter plot width vs length\n",
        "plt.scatter(width, length)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation\n",
        "correlation, pvalue = pearsonr(width, length)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6k2fb5JSWrP",
        "colab_type": "text"
      },
      "source": [
        "## Decorrelating the grain measurements with PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-ID3zf0SXBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Apply the fit_transform method of model to grains: pca_features\n",
        "pca_features = model.fit_transform(grains)\n",
        "\n",
        "# Assign 0th column of pca_features: xs\n",
        "xs = pca_features[:,0]\n",
        "\n",
        "# Assign 1st column of pca_features: ys\n",
        "ys = pca_features[:,1]\n",
        "\n",
        "# Scatter plot xs vs ys\n",
        "plt.scatter(xs, ys)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation of xs and ys\n",
        "correlation, pvalue = pearsonr(xs, ys)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zb-RqKTTLJm",
        "colab_type": "text"
      },
      "source": [
        "## Principal components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OghJ4IIvTLdo",
        "colab_type": "text"
      },
      "source": [
        "B. Both plot 1 and plot 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V97y9HNQTOZv",
        "colab_type": "text"
      },
      "source": [
        "## The first principal component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR62BDEHU0bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a scatter plot of the untransformed points\n",
        "plt.scatter(grains[:,0], grains[:,1])\n",
        "\n",
        "# Create a PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(grains)\n",
        "\n",
        "# Get the mean of the grain samples: mean\n",
        "mean = model.mean_\n",
        "\n",
        "# Get the first principal component: first_pc\n",
        "first_pc = model.components_[0, :]\n",
        "\n",
        "# Plot first_pc as an arrow, starting at mean\n",
        "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
        "\n",
        "# Keep axes on same scale\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAcdHf1kVvJp",
        "colab_type": "text"
      },
      "source": [
        "## Variance of the PCA features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVKQ7CTnVvgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a PCA instance: pca\n",
        "pca = PCA()\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, pca)\n",
        "\n",
        "# Fit the pipeline to 'samples'\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(pca.n_components_)\n",
        "plt.bar(features, pca.explained_variance_)\n",
        "plt.xlabel('PCA feature')\n",
        "plt.ylabel('variance')\n",
        "plt.xticks(features)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbw3v5qDWc1v",
        "colab_type": "text"
      },
      "source": [
        "## Intrinsic dimension of the fish data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNcZqxUbWdOD",
        "colab_type": "text"
      },
      "source": [
        "B. 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF7ahKmbWjs0",
        "colab_type": "text"
      },
      "source": [
        "## Dimension reduction of the fish measurements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv0Odi79XWmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA model with 2 components: pca\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit the PCA instance to the scaled samples\n",
        "pca.fit(scaled_samples)\n",
        "\n",
        "# Transform the scaled samples: pca_features\n",
        "pca_features = pca.transform(scaled_samples)\n",
        "\n",
        "# Print the shape of pca_features\n",
        "print(pca_features.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwJwl1V8XtDb",
        "colab_type": "text"
      },
      "source": [
        "## A tf-idf word-frequency array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NVxnRg2Xtbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer: tfidf\n",
        "tfidf = TfidfVectorizer() \n",
        "\n",
        "# Apply fit_transform to document: csr_mat\n",
        "csr_mat = tfidf.fit_transform(documents)\n",
        "\n",
        "# Print result of toarray() method\n",
        "print(csr_mat.toarray())\n",
        "\n",
        "# Get the words: words\n",
        "words = tfidf.get_feature_names()\n",
        "\n",
        "# Print words\n",
        "print(words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU3klYcHYREn",
        "colab_type": "text"
      },
      "source": [
        "## Clustering Wikipedia part I"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hvYsqiAYRbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a TruncatedSVD instance: svd\n",
        "svd = TruncatedSVD(n_components=50)\n",
        "\n",
        "# Create a KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters=6)\n",
        "\n",
        "# Create a pipeline: pipeline\n",
        "pipeline = make_pipeline(svd, kmeans)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aph8bP6gYtl-",
        "colab_type": "text"
      },
      "source": [
        "## Clustering Wikipedia part II"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BU99wnwYt8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to articles\n",
        "pipeline.fit(articles)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(articles)\n",
        "\n",
        "# Create a DataFrame aligning labels and titles: df\n",
        "df = pd.DataFrame({'label': labels, 'article': titles})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('label'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}