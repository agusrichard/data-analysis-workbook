{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learn_from_experts.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaEyO3FYFeTp",
        "colab_type": "text"
      },
      "source": [
        "# Learning from the experts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjRpg6_iFi7v",
        "colab_type": "text"
      },
      "source": [
        "## How many tokens?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZIC98pRFlGU",
        "colab_type": "text"
      },
      "source": [
        "B. 4, because , and & are not tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThEtpWt9F1zn",
        "colab_type": "text"
      },
      "source": [
        "## Deciding what's a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b52NDpgSF3_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the text vector\n",
        "text_vector = combine_text_columns(X_train)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate the CountVectorizer: text_features\n",
        "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit text_features to the text vector\n",
        "text_features.fit(text_vector)\n",
        "\n",
        "# Print the first 10 tokens\n",
        "print(text_features.get_feature_names()[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1SDOqxgGt0j",
        "colab_type": "text"
      },
      "source": [
        "## N-gram range in scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1qHz9TvGuOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Import classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Import other preprocessing modules\n",
        "from sklearn.preprocessing import Imputer\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "\n",
        "# Select 300 best features\n",
        "chi_k = 300\n",
        "\n",
        "# Import functional utilities\n",
        "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "# Perform preprocessing\n",
        "get_text_data = FunctionTransformer(combine_text_columns, validate=False)\n",
        "get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                   ngram_range=(1, 2))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwOTPjIHIil8",
        "colab_type": "text"
      },
      "source": [
        "## Which models of the data include interaction terms?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3oDYto2Ii-4",
        "colab_type": "text"
      },
      "source": [
        "B. The second expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrS4vIpDIs4q",
        "colab_type": "text"
      },
      "source": [
        "## Implement interaction modeling in scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHyt2VoyIvQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                   ngram_range=(1, 2))),  \n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kja7yydYKIpy",
        "colab_type": "text"
      },
      "source": [
        "## Why is hashing a useful trick?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bHyBawIKHOI",
        "colab_type": "text"
      },
      "source": [
        "C. Some problems are memory-bound and not easily parallelizable, and hashing enforces a fixed length computation instead of using a mutable datatype (like a dictionary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlVLaX9HKd-Z",
        "colab_type": "text"
      },
      "source": [
        "## Implementing the hashing trick in scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAEBByFBKjAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Get text data: text_data\n",
        "text_data = combine_text_columns(X_train)\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
        "\n",
        "# Instantiate the HashingVectorizer: hashing_vec\n",
        "hashing_vec = HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit and transform the Hashing Vectorizer\n",
        "hashed_text = hashing_vec.fit_transform(text_data)\n",
        "\n",
        "# Create DataFrame and print the head\n",
        "hashed_df = pd.DataFrame(hashed_text.data)\n",
        "print(hashed_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tvmpj8fK8Tr",
        "colab_type": "text"
      },
      "source": [
        "## Build the winning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLhn9JLRK8ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the hashing vectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "# Instantiate the winning model pipeline: pl\n",
        "pl = Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "            transformer_list = [\n",
        "                ('numeric_features', Pipeline([\n",
        "                    ('selector', get_numeric_data),\n",
        "                    ('imputer', Imputer())\n",
        "                ])),\n",
        "                ('text_features', Pipeline([\n",
        "                    ('selector', get_text_data),\n",
        "                    ('vectorizer', HashingVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
        "                                                     non_negative=True, norm=None, binary=False,\n",
        "                                                     ngram_range=(1, 2))),\n",
        "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
        "                ]))\n",
        "             ]\n",
        "        )),\n",
        "        ('int', SparseInteractions(degree=2)),\n",
        "        ('scale', MaxAbsScaler()),\n",
        "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy4X8cgaLVxJ",
        "colab_type": "text"
      },
      "source": [
        "## What tactics got the winner the best score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1XoY4jlLUsA",
        "colab_type": "text"
      },
      "source": [
        "C. The winner used skillful NLP, efficient computation, and simple but powerful stats tricks to master the budget data."
      ]
    }
  ]
}