{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "creating_simple_first_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL6vdduafWVu",
        "colab_type": "text"
      },
      "source": [
        "# Creating a simple first model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgAL8B2HfY3P",
        "colab_type": "text"
      },
      "source": [
        "## Setting up a train-test split in scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34tBxNBqfest",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the new DataFrame: numeric_data_only\n",
        "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
        "\n",
        "# Get labels and convert to dummy variables: label_dummies\n",
        "label_dummies = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
        "                                                               label_dummies,\n",
        "                                                               size=0.2, \n",
        "                                                               seed=123)\n",
        "\n",
        "# Print the info\n",
        "print(\"X_train info:\")\n",
        "print(X_train.info())\n",
        "print(\"\\nX_test info:\")  \n",
        "print(X_test.info())\n",
        "print(\"\\ny_train info:\")  \n",
        "print(y_train.info())\n",
        "print(\"\\ny_test info:\")  \n",
        "print(y_test.info()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfOFcTjGgREq",
        "colab_type": "text"
      },
      "source": [
        "## Training a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgdubPs8gRcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Create the DataFrame: numeric_data_only\n",
        "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
        "\n",
        "# Get labels and convert to dummy variables: label_dummies\n",
        "label_dummies = pd.get_dummies(df[LABELS])\n",
        "\n",
        "# Create training and test sets\n",
        "X_train, X_test, y_train, y_test = multilabel_train_test_split(numeric_data_only,\n",
        "                                                               label_dummies,\n",
        "                                                               size=0.2, \n",
        "                                                               seed=123)\n",
        "\n",
        "# Instantiate the classifier: clf\n",
        "clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSCY0HxgwJrw",
        "colab_type": "text"
      },
      "source": [
        "## Use your model to predict values on holdout data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGj42rvvwKD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the classifier: clf\n",
        "clf = OneVsRestClassifier(LogisticRegression())\n",
        "\n",
        "# Fit it to the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Load the holdout data: holdout\n",
        "holdout = pd.read_csv('HoldoutData.csv', index_col=0)\n",
        "\n",
        "# Generate predictions: predictions\n",
        "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kskp5LHpwrIt",
        "colab_type": "text"
      },
      "source": [
        "## Writing out your results to a csv for submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2DjXJXKwrgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate predictions: predictions\n",
        "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
        "\n",
        "# Format predictions in DataFrame: prediction_df\n",
        "prediction_df = pd.DataFrame(columns=pd.get_dummies(df[LABELS]).columns,\n",
        "                             index=holdout.index,\n",
        "                             data=predictions)\n",
        "\n",
        "\n",
        "# Save prediction_df to csv\n",
        "prediction_df.to_csv('predictions.csv')\n",
        "\n",
        "# Submit the predictions for scoring: score\n",
        "score = score_submission(pred_path='predictions.csv')\n",
        "\n",
        "# Print score\n",
        "print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZAfPBdQyaqe",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkHqKobbybEI",
        "colab_type": "text"
      },
      "source": [
        "B. 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or6bUNztyqHs",
        "colab_type": "text"
      },
      "source": [
        "## Testing your NLP credentials with n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU9rm9ZXys4S",
        "colab_type": "text"
      },
      "source": [
        "D. 12."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNVeUoLUy57_",
        "colab_type": "text"
      },
      "source": [
        "## Creating a bag-of-words in scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1uZ60Gjzv1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Fill missing values in df.Position_Extra\n",
        "df.Position_Extra.fillna('', inplace=True)\n",
        "\n",
        "# Instantiate the CountVectorizer: vec_alphanumeric\n",
        "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Fit to the data\n",
        "vec_alphanumeric.fit(df.Position_Extra)\n",
        "\n",
        "# Print the number of tokens and first 15 tokens\n",
        "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
        "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
        "print(vec_alphanumeric.get_feature_names()[:15])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuKeq95J0bhc",
        "colab_type": "text"
      },
      "source": [
        "## Combining text columns for tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D0fDzpG0b6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define combine_text_columns()\n",
        "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
        "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
        "    \n",
        "    # Drop non-text columns that are in the df\n",
        "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
        "    text_data = data_frame.drop(to_drop, axis=1)\n",
        "    \n",
        "    # Replace nans with blanks\n",
        "    text_data.fillna(\"\", inplace=True)\n",
        "    \n",
        "    # Join all text items in a row that have a space in between\n",
        "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO6OC6MS0s1L",
        "colab_type": "text"
      },
      "source": [
        "## What's in a token?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWcdl_lr0tO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create the basic token pattern\n",
        "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
        "\n",
        "# Create the alphanumeric token pattern\n",
        "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
        "\n",
        "# Instantiate basic CountVectorizer: vec_basic\n",
        "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
        "\n",
        "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
        "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
        "\n",
        "# Create the text vector\n",
        "text_vector = combine_text_columns(df)\n",
        "\n",
        "# Fit and transform vec_basic\n",
        "vec_basic.fit_transform(text_vector)\n",
        "\n",
        "# Print number of tokens of vec_basic\n",
        "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
        "\n",
        "# Fit and transform vec_alphanumeric\n",
        "vec_alphanumeric.fit_transform(text_vector)\n",
        "\n",
        "# Print number of tokens of vec_alphanumeric\n",
        "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}