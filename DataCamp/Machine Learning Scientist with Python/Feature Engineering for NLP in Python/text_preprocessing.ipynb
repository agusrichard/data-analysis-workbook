{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbdl-xZ6nHoC",
        "colab_type": "text"
      },
      "source": [
        "# Text preprocessing, POS tagging and NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGHEmJR_gQOG",
        "colab_type": "text"
      },
      "source": [
        "## Identifying lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU1D3YwqgQh6",
        "colab_type": "text"
      },
      "source": [
        "D. Car, Bike, Truck, Bus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXdgrVYEgd4T",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing the Gettysburg Address"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWzF0D5ugh0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate the tokens\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lin3HPmQhCvz",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatizing the Gettysburg address"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72vPiFJThDCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the gettysburg address\n",
        "print(gettysburg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSQLYa-yhMhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate lemmas\n",
        "lemmas = [token.lemma_ for token in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wNtnjJshaTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate lemmas\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Convert lemmas into a string\n",
        "print(' '.join(lemmas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn2xJcMuihY_",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning a blog post"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5g8svQ5ihx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load model and create Doc object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(blog)\n",
        "\n",
        "# Generate lemmatized tokens\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "\n",
        "# Print string after text cleaning\n",
        "print(' '.join(a_lemmas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB7DrC2Ki1sp",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning TED talks in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRNWyPhPi2AV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to preprocess text\n",
        "def preprocess(text):\n",
        "  \t# Create Doc object\n",
        "    doc = nlp(text, disable=['ner', 'parser'])\n",
        "    # Generate lemmas\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    # Remove stopwords and non-alphabetic characters\n",
        "    a_lemmas = [lemma for lemma in lemmas \n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "    \n",
        "    return ' '.join(a_lemmas)\n",
        "  \n",
        "# Apply preprocess to ted['transcript']\n",
        "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
        "print(ted['transcript'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6fmdNeyj6xy",
        "colab_type": "text"
      },
      "source": [
        "## POS tagging in Lord of the Flies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-Z-gNJmj7Ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(lotf)\n",
        "\n",
        "# Generate tokens and pos tags\n",
        "pos = [(token.text, token.pos_) for token in doc]\n",
        "print(pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thgMFaqckQMa",
        "colab_type": "text"
      },
      "source": [
        "## Counting nouns in a piece of text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J6ljyOlkQf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Returns number of proper nouns\n",
        "def proper_nouns(text, model=nlp):\n",
        "  \t# Create doc object\n",
        "    doc = model(text)\n",
        "    # Generate list of POS tags\n",
        "    pos = [token.pos_ for token in doc]\n",
        "    \n",
        "    # Return number of proper nouns\n",
        "    return pos.count('PROPN')\n",
        "\n",
        "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyp1NNIDkow1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Returns number of other nouns\n",
        "def nouns(text, model=nlp):\n",
        "  \t# Create doc object\n",
        "    doc = model(text)\n",
        "    # Generate list of POS tags\n",
        "    pos = [token.pos_ for token in doc]\n",
        "    \n",
        "    # Return number of other nouns\n",
        "    return pos.count('NOUN')\n",
        "\n",
        "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfCh-YAjktMg",
        "colab_type": "text"
      },
      "source": [
        "## Noun usage in fake news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BI57QDbktgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
        "headlines['num_noun'] = headlines['title'].apply(num_noun)\n",
        "\n",
        "# Compute mean of proper nouns\n",
        "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
        "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
        "\n",
        "# Compute mean of other nouns\n",
        "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
        "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
        "\n",
        "# Print results\n",
        "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
        "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWU4hNs6mDk9",
        "colab_type": "text"
      },
      "source": [
        "## Named entities in a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXArM4vDmEAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the required model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc instance \n",
        "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print all named entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DZcGPhmY7W",
        "colab_type": "text"
      },
      "source": [
        "## Identifying people mentioned in a news article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hes-TF5dmZN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_persons(text):\n",
        "  # Create Doc object\n",
        "  doc = nlp(text)\n",
        "  \n",
        "  # Identify the persons\n",
        "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
        "  \n",
        "  # Return persons\n",
        "  return persons\n",
        "\n",
        "print(find_persons(tc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}