{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improving_model_performance.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhZuWJBH8fan",
        "colab_type": "text"
      },
      "source": [
        "# Improving Your Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUpmBr_E9fhC",
        "colab_type": "text"
      },
      "source": [
        "## Learning the digits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JB9IWyt9j0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
        "model.add(Dense(16, input_shape = (64,), activation = 'relu'))\n",
        "\n",
        "# Output layer with 10 neurons (one per digit) and softmax\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile your model\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Test if your model works and can process input data\n",
        "print(model.predict(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyMgT28j-ECO",
        "colab_type": "text"
      },
      "source": [
        "## Is the model overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoDGCO7J-EZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
        "history = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "# Extract from the history object loss and val_loss to plot the learning curve\n",
        "plot_loss(history.history['loss'], history.history['val_loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c033EwX2-pc0",
        "colab_type": "text"
      },
      "source": [
        "B. No, the test loss is not getting higher as the epochs go by."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNLUO2tD-p7u",
        "colab_type": "text"
      },
      "source": [
        "## Do we need more data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbfoPPgd-uDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for size in training_sizes:\n",
        "  \t# Get a fraction of training data (we only care about the training data)\n",
        "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
        "\n",
        "    # Reset the model to the initial weights and train it on the new data fraction\n",
        "    model.set_weights(initial_weights)\n",
        "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
        "\n",
        "    # Evaluate and store the train fraction and the complete test set results\n",
        "    train_accs.append(model.evaluate(X_train, y_train)[1])\n",
        "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
        "    \n",
        "# Plot train vs test accuracies\n",
        "plot_results(train_accs, test_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UUfWTAx_8UA",
        "colab_type": "text"
      },
      "source": [
        "## Different activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0Zh4ldM_8rD",
        "colab_type": "text"
      },
      "source": [
        "C. The sigmoid() and tanh() both take values close to -1 for big negative numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a4lXr0JANdx",
        "colab_type": "text"
      },
      "source": [
        "## Comparing activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEY0YK_iAPbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activation functions to try\n",
        "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
        "\n",
        "# Loop over the activation functions\n",
        "activation_results = {}\n",
        "\n",
        "for act in activations:\n",
        "  # Get a new model with the current activation\n",
        "  model = get_model(act)\n",
        "  # Fit the model\n",
        "  history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, verbose=0)\n",
        "  activation_results[act] = history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTa1KzU3A1uZ",
        "colab_type": "text"
      },
      "source": [
        "## Comparing activation functions II"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ybGsPkGA2J1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dataframe from val_loss_per_function\n",
        "val_loss= pd.DataFrame(val_loss_per_function)\n",
        "\n",
        "# Call plot on the dataframe\n",
        "val_loss.plot()\n",
        "plt.show()\n",
        "\n",
        "# Create a dataframe from val_acc_per_function\n",
        "val_acc = pd.DataFrame(val_acc_per_function)\n",
        "\n",
        "# Call plot on the dataframe\n",
        "val_acc.plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wmBkz7qB9ij",
        "colab_type": "text"
      },
      "source": [
        "## Changing batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQrlvxcCB98V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a fresh new model with get_model\n",
        "model = get_model()\n",
        "\n",
        "# Train your model for 5 epochs with a batch size of 1\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=1)\n",
        "print(\"\\n The accuracy when using a batch of size 1 is: \",\n",
        "      model.evaluate(X_test, y_test)[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xzLuh0TCPtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = get_model()\n",
        "\n",
        "# Fit your model for 5 epochs with a batch of size the training set\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=X_train.shape[0])\n",
        "print(\"\\n The accuracy when using the whole training set as a batch was: \",\n",
        "      model.evaluate(X_test, y_test)[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvBgHbrqCYOk",
        "colab_type": "text"
      },
      "source": [
        "## Batch normalizing a familiar model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtFp9YJfCYm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import batch normalization from keras layers\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "# Build your deep network\n",
        "batchnorm_model = Sequential()\n",
        "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
        "batchnorm_model.add(BatchNormalization())\n",
        "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
        "batchnorm_model.add(BatchNormalization())\n",
        "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
        "batchnorm_model.add(BatchNormalization())\n",
        "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
        "\n",
        "# Compile your model with sgd\n",
        "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJxRraxwSgpD",
        "colab_type": "text"
      },
      "source": [
        "## Batch normalization effects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsLM-ItsShJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train your standard model, storing its history\n",
        "history1 = standard_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
        "\n",
        "# Train the batch normalized model you recently built, store its history\n",
        "history2 = batchnorm_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
        "\n",
        "# Call compare_acc_histories passing in both model histories\n",
        "compare_histories_acc(history1, history2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18foPNsvTxXV",
        "colab_type": "text"
      },
      "source": [
        "## Preparing a model for tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0iNskXcTxuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creates a model given an activation and learning rate\n",
        "def create_model(learning_rate=0.01, activation='relu'):\n",
        "  \n",
        "  \t# Create an Adam optimizer with the given learning rate\n",
        "  \topt = Adam(lr=learning_rate)\n",
        "  \t\n",
        "  \t# Create your binary classification model  \n",
        "  \tmodel = Sequential()\n",
        "  \tmodel.add(Dense(128, input_shape=(30,), activation=activation))\n",
        "  \tmodel.add(Dense(256, activation=activation))\n",
        "  \tmodel.add(Dense(1, activation='sigmoid'))\n",
        "  \t\n",
        "  \t# Compile your model with your optimizer, loss, and metrics\n",
        "  \tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  \treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT5GQgPNUCwn",
        "colab_type": "text"
      },
      "source": [
        "## Tuning the model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4nMnbjqUDH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import KerasClassifier from keras wrappers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Create a KerasClassifier\n",
        "model = KerasClassifier(build_fn = create_model)\n",
        "\n",
        "# Define the parameters to try out\n",
        "params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], \n",
        "          'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
        "\n",
        "# Create a randomize search cv object passing in the parameters to try\n",
        "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
        "\n",
        "# Running random_search.fit(X,y) would start the search,but it takes too long! \n",
        "show_results()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PatlZd-6UnTl",
        "colab_type": "text"
      },
      "source": [
        "## Training with cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk6zYfhiUnvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import KerasClassifier from keras wrappers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Create a KerasClassifier\n",
        "model = KerasClassifier(build_fn = create_model, epochs = 50, \n",
        "             batch_size = 128, verbose = 0)\n",
        "\n",
        "# Calculate the accuracy score for each fold\n",
        "kfolds = cross_val_score(model, X, y, cv = 3)\n",
        "\n",
        "# Print the mean accuracy\n",
        "print('The mean accuracy was:', kfolds.mean())\n",
        "\n",
        "# Print the accuracy standard deviation\n",
        "print('With a standard deviation of:', kfolds.std())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}