{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_topic_identification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hsl70CAXvp3",
        "colab_type": "text"
      },
      "source": [
        "# Simple topic identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVfMWWMSXCuk",
        "colab_type": "text"
      },
      "source": [
        "## Bag-of-words picker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIxHPlXmXt1v",
        "colab_type": "text"
      },
      "source": [
        "D. ('The', 2), ('box', 2), ('.', 2), ('cat', 2), ('is', 1), ('in', 1), ('the', 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbniVXmEX6k5",
        "colab_type": "text"
      },
      "source": [
        "## Building a Counter with bag-of-words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1YDrvG5X8dp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Counter\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqURsbMvZPTP",
        "colab_type": "text"
      },
      "source": [
        "## Text preprocessing steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvHMTfWWZPrq",
        "colab_type": "text"
      },
      "source": [
        "B. Lemmatization, lowercasing, removing unwanted tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xhXq7B5ZWMk",
        "colab_type": "text"
      },
      "source": [
        "## Text preprocessing practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptij72hSZYrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in english_stops]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcIj4HU3a0Fl",
        "colab_type": "text"
      },
      "source": [
        "## What are word vectors?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDZuJ8dja0h5",
        "colab_type": "text"
      },
      "source": [
        "C. Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPHUcbNUbAiG",
        "colab_type": "text"
      },
      "source": [
        "## Creating and querying a corpus with gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BuMDIQcbH8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Dictionary\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(articles)\n",
        "\n",
        "# Select the id for \"computer\": computer_id\n",
        "computer_id = dictionary.token2id.get(\"computer\")\n",
        "\n",
        "# Use computer_id with the dictionary to print the word\n",
        "print(dictionary.get(computer_id))\n",
        "\n",
        "# Create a MmCorpus: corpus\n",
        "corpus = [dictionary.doc2bow(article) for article in articles]\n",
        "\n",
        "# Print the first 10 word ids with their frequency counts from the fifth document\n",
        "print(corpus[4][:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffyiS-cVcCaq",
        "colab_type": "text"
      },
      "source": [
        "## Gensim bag-of-words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NznL89wcCxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the fifth document: doc\n",
        "doc = corpus[4]\n",
        "\n",
        "# Sort the doc for frequency: bow_doc\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words of the document alongside the count\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "    \n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[word_id] += word_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8orgVsuCdMF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the fifth document: doc\n",
        "doc = corpus[4]\n",
        "\n",
        "# Sort the doc for frequency: bow_doc\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words of the document alongside the count\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "    \n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[word_id] += word_count\n",
        "    \n",
        "# Create a sorted list from the defaultdict: sorted_word_count\n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
        "\n",
        "# Print the top 5 words across all documents alongside the count\n",
        "for word_id, word_count in sorted_word_count[:5]:\n",
        "    print(dictionary.get(word_id), word_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-URMuZ8d6Ne",
        "colab_type": "text"
      },
      "source": [
        "## What is tf-idf?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h43Z1iL6d6lL",
        "colab_type": "text"
      },
      "source": [
        "A. (5 / 100) * log(200 / 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SQBVrBseNIx",
        "colab_type": "text"
      },
      "source": [
        "## Tf-idf with Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCehlVuBePYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new TfidfModel using the corpus: tfidf\n",
        "tfidf = TfidfModel(corpus)\n",
        "\n",
        "# Calculate the tfidf weights of doc: tfidf_weights\n",
        "tfidf_weights = tfidf[doc]\n",
        "\n",
        "# Print the first five weights\n",
        "print(tfidf_weights[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1QV8fkFes-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new TfidfModel using the corpus: tfidf\n",
        "tfidf = TfidfModel(corpus)\n",
        "\n",
        "# Calculate the tfidf weights of doc: tfidf_weights\n",
        "tfidf_weights = tfidf[doc]\n",
        "\n",
        "# Print the first five weights\n",
        "print(tfidf_weights[:5])\n",
        "\n",
        "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
        "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 weighted words\n",
        "for term_id, weight in sorted_tfidf_weights[:5]:\n",
        "    print(dictionary.get(term_id), weight)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}