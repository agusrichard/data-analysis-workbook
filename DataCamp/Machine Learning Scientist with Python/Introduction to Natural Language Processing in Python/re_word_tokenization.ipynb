{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "re_word_tokenization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1CKZ4tvNyOT",
        "colab_type": "text"
      },
      "source": [
        "# Regular expressions & word tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf_8FL1COXOP",
        "colab_type": "text"
      },
      "source": [
        "## Which pattern?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7lVFmN2PJ2r",
        "colab_type": "text"
      },
      "source": [
        "B. PATTERN = r\"\\w+\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xowXZohPXPF",
        "colab_type": "text"
      },
      "source": [
        "## Practicing regular expressions: re.split() and re.findall()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1So_N_aSPZPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "sentence_endings = r\"[.!?]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings, my_string))\n",
        "\n",
        "# Find all capitalized words in my_string and print the result\n",
        "capitalized_words = r\"[A-Z]\\w+\"\n",
        "print(re.findall(capitalized_words, my_string))\n",
        "\n",
        "# Split my_string on spaces and print the result\n",
        "spaces = r\"\\s+\"\n",
        "print(re.split(spaces, my_string))\n",
        "\n",
        "# Find all digits in my_string and print the result\n",
        "digits = r\"\\d+\"\n",
        "print(re.findall(digits, my_string))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3FlT41ZQyVq",
        "colab_type": "text"
      },
      "source": [
        "## Word tokenization with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y37lp0mQys3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary modules\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZzKFhSERXBK",
        "colab_type": "text"
      },
      "source": [
        "## More regex with re.search()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lANXwaBlRXXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "\n",
        "# Print the start and end indexes of match\n",
        "print(match.start(), match.end())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKIHek1mR7x1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write a regular expression to search for anything in square brackets: pattern1\n",
        "pattern1 = r\"\\[.*\\]\"\n",
        "\n",
        "# Use re.search to find the first text in square brackets\n",
        "print(re.search(pattern1, scene_one))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt3iFsdhSW32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the script notation at the beginning of the fourth sentence and print it\n",
        "pattern2 = r\"[a-zA-Z\\s#0-1]+:\"\n",
        "print(re.match(pattern2, sentences[3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6TsbrO4TGP2",
        "colab_type": "text"
      },
      "source": [
        "## Choosing a tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAfoFAe6TGnC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LXA9MnKUGrq",
        "colab_type": "text"
      },
      "source": [
        "B. r\"(\\w+|#\\d|\\?|!)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJHA1oe3UIOP",
        "colab_type": "text"
      },
      "source": [
        "## Regex with NLTK tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf310YIpUJ_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdGzPmTIUo6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Define a regex pattern to find hashtags: pattern1\n",
        "pattern1 = r\"#\\w+\"\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
        "print(hashtags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43pFwsgiU3r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Write a pattern that matches both mentions (@) and hashtags\n",
        "pattern2 = r\"([@#]\\w+)\"\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
        "print(mentions_hashtags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTldKoD2VFYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvlt9aEwVJzO",
        "colab_type": "text"
      },
      "source": [
        "## Non-ascii tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT8VGoDqVKJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize and print all words in german_text\n",
        "all_words = word_tokenize(german_text)\n",
        "print(all_words)\n",
        "\n",
        "# Tokenize and print only capital words\n",
        "capital_words = r\"[A-ZÃœ]\\w+\"\n",
        "print(regexp_tokenize(german_text, capital_words))\n",
        "\n",
        "# Tokenize and print only emoji\n",
        "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
        "print(regexp_tokenize(german_text, emoji))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8S1gfhRWXra",
        "colab_type": "text"
      },
      "source": [
        "## Charting practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8A6SKqeWYB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the script into lines: lines\n",
        "lines = holy_grail.split('\\n')\n",
        "\n",
        "# Replace all script lines for speaker\n",
        "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
        "lines = [re.sub(pattern, '', l) for l in lines]\n",
        "\n",
        "# Tokenize each line: tokenized_lines\n",
        "tokenized_lines = [regexp_tokenize(s, r'\\w+') for s in lines]\n",
        "\n",
        "# Make a frequency list of lengths: line_num_words\n",
        "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
        "\n",
        "# Plot a histogram of the line lengths\n",
        "plt.hist(line_num_words)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}