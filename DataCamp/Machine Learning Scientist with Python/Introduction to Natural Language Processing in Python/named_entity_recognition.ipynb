{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "named_entity_recognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AozoudIqkBgG",
        "colab_type": "text"
      },
      "source": [
        "# Named-entity recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdBdZ3WIe6B8",
        "colab_type": "text"
      },
      "source": [
        "## NER with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT453dKmj_sI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize the article into sentences: sentences\n",
        "sentences = sent_tokenize(article)\n",
        "\n",
        "# Tokenize each sentence into words: token_sentences\n",
        "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
        "\n",
        "# Create the named entity chunks: chunked_sentences\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
        "\n",
        "# Test for stems of the tree with 'NE' tags\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
        "            print(chunk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdhOgGZvk3Zs",
        "colab_type": "text"
      },
      "source": [
        "## Charting practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z1p8yKkk3wH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the defaultdict: ner_categories\n",
        "ner_categories = defaultdict(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLDtLR6Zlj7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the defaultdict: ner_categories\n",
        "ner_categories = defaultdict(int)\n",
        "\n",
        "# Create the nested for loop\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            ner_categories[chunk.label()] += 1\n",
        "            \n",
        "# Create a list from the dictionary keys for the chart labels: labels\n",
        "labels = list(ner_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx_isH4bl0bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the defaultdict: ner_categories\n",
        "ner_categories = defaultdict(int)\n",
        "\n",
        "# Create the nested for loop\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            ner_categories[chunk.label()] += 1\n",
        "            \n",
        "# Create a list from the dictionary keys for the chart labels: labels\n",
        "labels = list(ner_categories.keys())\n",
        "\n",
        "# Create a list of the values: values\n",
        "values = [ner_categories.get(v) for v in labels]\n",
        "\n",
        "# Create the pie chart\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG3iMQ5gl33C",
        "colab_type": "text"
      },
      "source": [
        "## Stanford library with NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riFSizYJl4PR",
        "colab_type": "text"
      },
      "source": [
        "D. NLTK, the Stanford Java Libraries and some environment variables to help with integration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS03Yuo-mIMl",
        "colab_type": "text"
      },
      "source": [
        "## Comparing NLTK with spaCy NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY8Si0c0mwlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import spacy\n",
        "import spacy\n",
        "\n",
        "# Instantiate the English model: nlp\n",
        "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
        "\n",
        "# Create a new document: doc\n",
        "doc = nlp(article)\n",
        "\n",
        "# Print all of the found entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_, ent.text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSx5FhhUoFsB",
        "colab_type": "text"
      },
      "source": [
        "## spaCy NER Categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7qjcAnzoJdn",
        "colab_type": "text"
      },
      "source": [
        "C. NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE, EVENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35z-cDZVoLSO",
        "colab_type": "text"
      },
      "source": [
        "## French NER with polyglot I"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdWKpX-AosUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new text object using Polyglot's Text class: txt\n",
        "txt = Text(article)\n",
        "\n",
        "# Print each of the entities found\n",
        "for ent in txt.entities:\n",
        "    print(ent)\n",
        "    \n",
        "# Print the type of ent\n",
        "print(type(ent))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO2dcRDWpqQQ",
        "colab_type": "text"
      },
      "source": [
        "## French NER with polyglot II"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m2FWeajpqnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the list of tuples: entities\n",
        "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
        "\n",
        "# Print entities\n",
        "print(entities)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf3jZE5Kp9uu",
        "colab_type": "text"
      },
      "source": [
        "## Spanish NER with polyglot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9gkB8mvp-GE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the count variable: count\n",
        "count = 0\n",
        "\n",
        "# Iterate over all the entities\n",
        "for ent in txt.entities:\n",
        "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
        "    if 'Márquez' in ent or 'Gabo' in ent:\n",
        "        # Increment count\n",
        "        count += 1\n",
        "\n",
        "# Print count\n",
        "print(count)\n",
        "\n",
        "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
        "percentage = count / len(txt.entities)\n",
        "print(percentage)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}