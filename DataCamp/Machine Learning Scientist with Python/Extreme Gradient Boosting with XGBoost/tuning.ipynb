{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tryhAvVVL5Py",
        "colab_type": "text"
      },
      "source": [
        "# Fine-tuning your XGBoost model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEX-UBL1MCit",
        "colab_type": "text"
      },
      "source": [
        "## When is tuning your model a bad idea?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZI_uwo-MsVe",
        "colab_type": "text"
      },
      "source": [
        "B. You are very short on time before you must push an initial model to production and have little data to train your model on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIDqo45tM4Cr",
        "colab_type": "text"
      },
      "source": [
        "## Tuning the number of boosting rounds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQV2NnqqM6LJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary for each tree: params \n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
        "\n",
        "# Create list of number of boosting rounds\n",
        "num_rounds = [5, 10, 15]\n",
        "\n",
        "# Empty list to store final round rmse per XGBoost model\n",
        "final_rmse_per_round = []\n",
        "\n",
        "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
        "for curr_num_rounds in num_rounds:\n",
        "\n",
        "    # Perform cross-validation: cv_results\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append final round RMSE\n",
        "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
        "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCD0UrH-NjpD",
        "colab_type": "text"
      },
      "source": [
        "## Automated boosting round selection using early_stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYdJgtYoNkAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your housing DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary for each tree: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation with early stopping: cv_results\n",
        "cv_results = xgb.cv(params=params, dtrain=housing_dmatrix, metrics='rmse', early_stopping_rounds=10, num_boost_round=50, seed=123, nfold=3, as_pandas=True)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_erjm9vUPCdq",
        "colab_type": "text"
      },
      "source": [
        "## Tuning eta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3D-zZs-PCz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your housing DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary for each tree (boosting round)\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
        "\n",
        "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
        "eta_vals = [0.001, 0.01, 0.1]\n",
        "best_rmse = []\n",
        "\n",
        "# Systematically vary the eta \n",
        "for curr_val in eta_vals:\n",
        "\n",
        "    params[\"eta\"] = curr_val\n",
        "    \n",
        "    # Perform cross-validation: cv_results\n",
        "    cv_results = xgb.cv(params=params, dtrain=housing_dmatrix, early_stopping_rounds=5, nfold=3, num_boost_round=10, metrics='rmse', seed=123, as_pandas=True)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Append the final round rmse to best_rmse\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eFslcGnPtwr",
        "colab_type": "text"
      },
      "source": [
        "## Tuning max_depth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfQf4y5xPuIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your housing DMatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "# Create the parameter dictionary\n",
        "params = {\"objective\":\"reg:linear\"}\n",
        "\n",
        "# Create list of max_depth values\n",
        "max_depths = [2, 5, 10, 20]\n",
        "best_rmse = []\n",
        "\n",
        "# Systematically vary the max_depth\n",
        "for curr_val in max_depths:\n",
        "\n",
        "    params[\"max_depth\"] = curr_val\n",
        "    \n",
        "    # Perform cross-validation\n",
        "    cv_results = xgb.cv(params=params, dtrain=housing_dmatrix, nfold=2, num_boost_round=10, early_stopping_rounds=5, metrics='rmse', seed=123, as_pandas=True)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Append the final round rmse to best_rmse\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxOiuXLqQKTL",
        "colab_type": "text"
      },
      "source": [
        "## Tuning colsample_bytree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRGpAvUsQKsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your housing DMatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
        "\n",
        "# Create the parameter dictionary\n",
        "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
        "\n",
        "# Create list of hyperparameter values: colsample_bytree_vals\n",
        "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
        "best_rmse = []\n",
        "\n",
        "# Systematically vary the hyperparameter value \n",
        "for curr_val in colsample_bytree_vals:\n",
        "\n",
        "    params['colsample_bytree'] = curr_val\n",
        "    \n",
        "    # Perform cross-validation\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
        "                 num_boost_round=10, early_stopping_rounds=5,\n",
        "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append the final round rmse to best_rmse\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VZ2bc_tRtV2",
        "colab_type": "text"
      },
      "source": [
        "## Grid search with XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXArSytORtEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the parameter grid: gbm_param_grid\n",
        "gbm_param_grid = {\n",
        "    'colsample_bytree': [0.3, 0.7],\n",
        "    'n_estimators': [50],\n",
        "    'max_depth': [2, 5]\n",
        "}\n",
        "\n",
        "# Instantiate the regressor: gbm\n",
        "gbm = xgb.XGBRegressor()\n",
        "\n",
        "# Perform grid search: grid_mse\n",
        "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, cv=4, scoring='neg_mean_squared_error', verbose=1)\n",
        "\n",
        "\n",
        "# Fit grid_mse to the data\n",
        "grid_mse.fit(X, y)\n",
        "\n",
        "# Print the best parameters and lowest RMSE\n",
        "print(\"Best parameters found: \", grid_mse.best_params_)\n",
        "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mrsekQqSeaL",
        "colab_type": "text"
      },
      "source": [
        "## Random search with XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlpRlEkCSe3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the parameter grid: gbm_param_grid \n",
        "gbm_param_grid = {\n",
        "    'n_estimators': [25],\n",
        "    'max_depth': range(2, 12)\n",
        "}\n",
        "\n",
        "# Instantiate the regressor: gbm\n",
        "gbm = xgb.XGBRegressor(n_estimators=10)\n",
        "\n",
        "# Perform random search: grid_mse\n",
        "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid, cv=4, n_iter=5, scoring='neg_mean_squared_error', verbose=1)\n",
        "\n",
        "\n",
        "# Fit randomized_mse to the data\n",
        "randomized_mse.fit(X, y)\n",
        "\n",
        "# Print the best parameters and lowest RMSE\n",
        "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
        "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av6WWSTPTNcK",
        "colab_type": "text"
      },
      "source": [
        "## When should you use grid search and random search?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZG6_yU-TNyu",
        "colab_type": "text"
      },
      "source": [
        "C. The search space size can be massive for Grid Search in certain cases, whereas for Random Search the number of hyperparameters has a significant effect on how long it takes to run."
      ]
    }
  ]
}