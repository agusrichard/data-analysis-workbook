{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_networks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFlqnAmfwVuT",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW-VfvZewYiQ",
        "colab_type": "text"
      },
      "source": [
        "## The linear algebra of dense layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJeJFpGlwc-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize bias1\n",
        "bias1 = Variable(1.0)\n",
        "\n",
        "# Initialize weights1 as 3x2 variable of ones\n",
        "weights1 = Variable(ones((3, 2)))\n",
        "\n",
        "# Perform matrix multiplication of borrower_features and weights1\n",
        "product1 = matmul(borrower_features, weights1)\n",
        "\n",
        "# Apply sigmoid activation function to product1 + bias1\n",
        "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
        "\n",
        "# Print shape of dense1\n",
        "print(\"\\n dense1's output shape: {}\".format(dense1.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EDw-KpFxas0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From previous step\n",
        "bias1 = Variable(1.0)\n",
        "weights1 = Variable(ones((3, 2)))\n",
        "product1 = matmul(borrower_features, weights1)\n",
        "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
        "\n",
        "# Initialize bias2 and weights2\n",
        "bias2 = Variable(1.0)\n",
        "weights2 = Variable(ones((2, 1)))\n",
        "\n",
        "# Perform matrix multiplication of dense1 and weights2\n",
        "product2 = matmul(dense1, weights2)\n",
        "\n",
        "# Apply activation to product2 + bias2 and print the prediction\n",
        "prediction = keras.activations.sigmoid(product2 + bias2)\n",
        "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
        "print('\\n actual: 1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8BRjrnGxiHG",
        "colab_type": "text"
      },
      "source": [
        "## The low-level approach with multiple examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjZwXYeYxigt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the product of borrower_features and weights1\n",
        "products1 = matmul(borrower_features, weights1)\n",
        "\n",
        "# Apply a sigmoid activation function to products1 + bias1\n",
        "dense1 = keras.activations.sigmoid(products1 + bias1)\n",
        "\n",
        "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
        "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
        "print('\\n shape of weights1: ', weights1.shape)\n",
        "print('\\n shape of bias1: ', bias1.shape)\n",
        "print('\\n shape of dense1: ', dense1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6KE6X1lyFvd",
        "colab_type": "text"
      },
      "source": [
        "## Using the dense layer operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0C4PD20yGaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the first dense layer\n",
        "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
        "\n",
        "# Define a dense layer with 3 output nodes\n",
        "dense2 = keras.layers.Dense(3, activation='sigmoid')(dense1)\n",
        "\n",
        "# Define a dense layer with 1 output node\n",
        "predictions = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
        "\n",
        "# Print the shapes of dense1, dense2, and predictions\n",
        "print('\\n shape of dense1: ', dense1.shape)\n",
        "print('\\n shape of dense2: ', dense2.shape)\n",
        "print('\\n shape of predictions: ', predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhp9_GZhzaYY",
        "colab_type": "text"
      },
      "source": [
        "## Binary classification problems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0aptyWWza3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct input layer from features\n",
        "inputs = constant(bill_amounts, float32)\n",
        "\n",
        "# Define first dense layer\n",
        "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
        "\n",
        "# Define second dense layer\n",
        "dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
        "\n",
        "# Define output layer\n",
        "outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
        "\n",
        "# Print error for first five examples\n",
        "error = default[:5] - outputs.numpy()[:5]\n",
        "print(error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HYL7thlz9SF",
        "colab_type": "text"
      },
      "source": [
        "## Multiclass classification problems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8Bo9Yt-z9tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct input layer from borrower features\n",
        "inputs = constant(borrower_features, float32)\n",
        "\n",
        "# Define first dense layer\n",
        "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
        "\n",
        "# Define second dense layer\n",
        "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
        "\n",
        "# Define output layer\n",
        "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
        "\n",
        "# Print first five predictions\n",
        "print(outputs.numpy()[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwpBQDcO13mU",
        "colab_type": "text"
      },
      "source": [
        "## The dangers of local minima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2_L719I14AS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize x_1 and x_2\n",
        "x_1 = Variable(6.0,float32)\n",
        "x_2 = Variable(0.3,float32)\n",
        "\n",
        "# Define the optimization operation\n",
        "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "for j in range(100):\n",
        "\t# Perform minimization using the loss function and x_1\n",
        "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
        "\t# Perform minimization using the loss function and x_2\n",
        "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
        "\n",
        "# Print x_1 and x_2 as numpy arrays\n",
        "print(x_1.numpy(), x_2.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHE1Zfmb2clG",
        "colab_type": "text"
      },
      "source": [
        "## Avoiding local minima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_YFGvPK2dDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize x_1 and x_2\n",
        "x_1 = Variable(0.05,float32)\n",
        "x_2 = Variable(0.05,float32)\n",
        "\n",
        "# Define the optimization operation for opt_1 and opt_2\n",
        "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
        "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0)\n",
        "\n",
        "for j in range(100):\n",
        "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
        "    # Define the minimization operation for opt_2\n",
        "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
        "\n",
        "# Print x_1 and x_2 as numpy arrays\n",
        "print(x_1.numpy(), x_2.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjMm7HJT3zWP",
        "colab_type": "text"
      },
      "source": [
        "## Initialization in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq9uuzHC3zz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the layer 1 weights\n",
        "w1 = Variable(random.normal([23, 7]))\n",
        "\n",
        "# Initialize the layer 1 bias\n",
        "b1 = Variable(ones([7]))\n",
        "\n",
        "# Define the layer 2 weights\n",
        "w2 = Variable(random.normal([7, 1]))\n",
        "\n",
        "# Define the layer 2 bias\n",
        "b2 = Variable(0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3lhgsOP4Jom",
        "colab_type": "text"
      },
      "source": [
        "## Defining the model and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ij4yPNw4KEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model\n",
        "def model(w1, b1, w2, b2, features = borrower_features):\n",
        "\t# Apply relu activation functions to layer 1\n",
        "\tlayer1 = keras.activations.relu(matmul(features, w1) + b1)\n",
        "    # Apply dropout\n",
        "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
        "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
        "\tpredictions = model(w1, b1, w2, b2)\n",
        "\t# Pass targets and predictions to the cross entropy loss\n",
        "\treturn keras.losses.binary_crossentropy(targets, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZbfa_FD4fvw",
        "colab_type": "text"
      },
      "source": [
        "## Training neural networks with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB4SI1Q94gRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "for j in range(100):\n",
        "    # Complete the optimizer\n",
        "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
        "                 var_list=[w1, b1, w2, b2])\n",
        "\n",
        "# Make predictions with model\n",
        "model_predictions = model(w1, b1, w2, b2, test_features)\n",
        "\n",
        "# Construct the confusion matrix\n",
        "confusion_matrix(test_targets, model_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}