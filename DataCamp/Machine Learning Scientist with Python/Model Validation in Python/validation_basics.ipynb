{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "validation_basics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfEsRqAW3IAZ",
        "colab_type": "text"
      },
      "source": [
        "# Validation Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bh4aOYx3cm4",
        "colab_type": "text"
      },
      "source": [
        "## Create one holdout set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhRmiJLB3J3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dummy variables using pandas\n",
        "X = pd.get_dummies(tic_tac_toe.iloc[:,0:9])\n",
        "y = tic_tac_toe.iloc[:, 9]\n",
        "\n",
        "# Create training and testing datasets. Use 10% for the test set\n",
        "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.1, random_state=1111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MtiXJ413fpG",
        "colab_type": "text"
      },
      "source": [
        "## Create two holdout sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaK2QLls3gS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create temporary training and final testing datasets\n",
        "X_temp, X_test, y_temp, y_test  =\\\n",
        "    train_test_split(X, y, test_size=0.20, random_state=1111)\n",
        "\n",
        "# Create the final training and validation datasets\n",
        "X_train, X_val, y_train, y_val  =\\\n",
        "    train_test_split(X_temp, y_temp, test_size=0.25, random_state=1111)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-VZqW2j4BOb",
        "colab_type": "text"
      },
      "source": [
        "## Why use holdout sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5ToJxBO4Bs_",
        "colab_type": "text"
      },
      "source": [
        "B. When testing parameters, tuning hyper-parameters, or anytime you are frequently evaluating model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M38oFeZv4KXE",
        "colab_type": "text"
      },
      "source": [
        "## Mean absolute error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upOqGOgO49Js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Manually calculate the MAE\n",
        "n = len(predictions)\n",
        "mae_one = sum(abs(y_test - predictions)) / n\n",
        "print('With a manual calculation, the error is {}'.format(mae_one))\n",
        "\n",
        "# Use scikit-learn to calculate the MAE\n",
        "mae_two = mean_absolute_error(y_test, predictions)\n",
        "print('Using scikit-lean, the error is {}'.format(mae_two))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjsTS7Zz5T-k",
        "colab_type": "text"
      },
      "source": [
        "## Mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5TnXLj85UXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "n = len(predictions)\n",
        "# Finish the manual calculation of the MSE\n",
        "mse_one = sum((y_test - predictions)**2) / n\n",
        "print('With a manual calculation, the error is {}'.format(mse_one))\n",
        "\n",
        "# Use the scikit-learn function to calculate MSE\n",
        "mse_two = mean_squared_error(y_test, predictions)\n",
        "print('Using scikit-lean, the error is {}'.format(mse_two))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjT8_c0k5puq",
        "colab_type": "text"
      },
      "source": [
        "## Performance on data subsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1kAavMj5qGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the East conference teams\n",
        "east_teams = labels == 'E'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPj-4mSk6Bcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the East conference teams\n",
        "east_teams = labels == \"E\"\n",
        "\n",
        "# Create arrays for the true and predicted values\n",
        "true_east = y_test[east_teams]\n",
        "preds_east = predictions[east_teams]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UwVoSdr6MKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the East conference teams\n",
        "east_teams = labels == \"E\"\n",
        "\n",
        "# Create arrays for the true and predicted values\n",
        "true_east = y_test[east_teams]\n",
        "preds_east = predictions[east_teams]\n",
        "\n",
        "# Print the accuracy metrics\n",
        "print('The MAE for East teams is {}'.format(\n",
        "    mae(true_east, preds_east)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1snxMOcd6U_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the East conference teams\n",
        "east_teams = labels == \"E\"\n",
        "\n",
        "# Create arrays for the true and predicted values\n",
        "true_east = y_test[east_teams]\n",
        "preds_east = predictions[east_teams]\n",
        "\n",
        "# Print the accuracy metrics\n",
        "print('The MAE for East teams is {}'.format(\n",
        "    mae(true_east, preds_east)))\n",
        "\n",
        "# Print the West accuracy\n",
        "print('The MAE for West conference is {}'.format(west_error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMWq1XTp7UMc",
        "colab_type": "text"
      },
      "source": [
        "## Confusion matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QofGmj_7UkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate and print the accuracy\n",
        "accuracy = (324 + 491) / (953)\n",
        "print(\"The overall accuracy is {0: 0.2f}\".format(accuracy))\n",
        "\n",
        "# Calculate and print the precision\n",
        "precision = (491) / (491 + 15)\n",
        "print(\"The precision is {0: 0.2f}\".format(precision))\n",
        "\n",
        "# Calculate and print the recall\n",
        "recall = (491) / (491 + 123)\n",
        "print(\"The recall is {0: 0.2f}\".format(recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNOnUK8D7y2a",
        "colab_type": "text"
      },
      "source": [
        "## Confusion matrices, again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl3yU4O27zO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create predictions\n",
        "test_predictions = rfc.predict(X_test)\n",
        "\n",
        "# Create and print the confusion matrix\n",
        "cm = confusion_matrix(y_test, test_predictions)\n",
        "print(cm)\n",
        "\n",
        "# Print the true positives (actual 1s that were predicted 1s)\n",
        "print(\"The number of true positives is: {}\".format(cm[1, 1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfho6Qeo8HL9",
        "colab_type": "text"
      },
      "source": [
        "## Precision vs. recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmwjO5jm8HkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "test_predictions = rfc.predict(X_test)\n",
        "\n",
        "# Create precision or recall score based on the metric you imported\n",
        "score = precision_score(y_test, test_predictions)\n",
        "\n",
        "# Print the final result\n",
        "print(\"The precision value is {0:.2f}\".format(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe9kY65R9SAD",
        "colab_type": "text"
      },
      "source": [
        "## Error due to under/over-fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PekqweXs9SX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Update the rfr model\n",
        "rfr = RandomForestRegressor(n_estimators=25,\n",
        "                            random_state=1111,\n",
        "                            max_features=2)\n",
        "rfr.fit(X_train, y_train)\n",
        "\n",
        "# Print the training and testing accuracies \n",
        "print('The training error is {0:.2f}'.format(\n",
        "  mae(y_train, rfr.predict(X_train))))\n",
        "print('The testing error is {0:.2f}'.format(\n",
        "  mae(y_test, rfr.predict(X_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqHNetK49mIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Update the rfr model\n",
        "rfr = RandomForestRegressor(n_estimators=25,\n",
        "                            random_state=1111,\n",
        "                            max_features=2)\n",
        "rfr.fit(X_train, y_train)\n",
        "\n",
        "# Print the training and testing accuracies \n",
        "print('The training error is {0:.2f}'.format(\n",
        "  mae(y_train, rfr.predict(X_train))))\n",
        "print('The testing error is {0:.2f}'.format(\n",
        "  mae(y_test, rfr.predict(X_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds_tCr249rZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Update the rfr model\n",
        "rfr = RandomForestRegressor(n_estimators=25,\n",
        "                            random_state=1111,\n",
        "                            max_features=2)\n",
        "rfr.fit(X_train, y_train)\n",
        "\n",
        "# Print the training and testing accuracies \n",
        "print('The training error is {0:.2f}'.format(\n",
        "  mae(y_train, rfr.predict(X_train))))\n",
        "print('The testing error is {0:.2f}'.format(\n",
        "  mae(y_test, rfr.predict(X_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KjzBuH99vUh",
        "colab_type": "text"
      },
      "source": [
        "## Am I underfitting?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9_AKDn09vsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "test_scores, train_scores = [], []\n",
        "for i in [1, 2, 3, 4, 5, 10, 20, 50]:\n",
        "    rfc = RandomForestClassifier(n_estimators=i, random_state=1111)\n",
        "    rfc.fit(X_train, y_train)\n",
        "    # Create predictions for the X_train and X_test datasets.\n",
        "    train_predictions = rfc.predict(X_train)\n",
        "    test_predictions = rfc.predict(X_test)\n",
        "    # Append the accuracy score for the test and train predictions.\n",
        "    train_scores.append(round(accuracy_score(y_train, train_predictions), 2))\n",
        "    test_scores.append(round(accuracy_score(y_test, test_predictions), 2))\n",
        "# Print the train and test scores.\n",
        "print(\"The training scores were: {}\".format(train_scores))\n",
        "print(\"The testing scores were: {}\".format(test_scores))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}