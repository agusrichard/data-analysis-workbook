{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "loss_functions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8jLhV0nn3LA",
        "colab_type": "text"
      },
      "source": [
        "# Loss functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h5g8Kz9nCRp",
        "colab_type": "text"
      },
      "source": [
        "## How models make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj3m06cwnz97",
        "colab_type": "text"
      },
      "source": [
        "D.Both logistic regression and Linear SVMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhT1kWdXn73Z",
        "colab_type": "text"
      },
      "source": [
        "## Changing the model coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxFtQ_bvn9nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the coefficients\n",
        "model.coef_ = np.array([[0,1]])\n",
        "model.intercept_ = np.array([0])\n",
        "\n",
        "# Plot the data and decision boundary\n",
        "plot_classifier(X,y,model)\n",
        "\n",
        "# Print the number of errors\n",
        "num_err = np.sum(y != model.predict(X))\n",
        "print(\"Number of errors:\", num_err)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9MSRco1o-85",
        "colab_type": "text"
      },
      "source": [
        "## The 0-1 loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcdchSd6o_SX",
        "colab_type": "text"
      },
      "source": [
        "C. 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At6Jco8UpJm3",
        "colab_type": "text"
      },
      "source": [
        "## Minimizing a loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPBCsT-3pLT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The squared error, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(y.size):\n",
        "        # Get the true and predicted target values for example 'i'\n",
        "        y_i_true = y[i]\n",
        "        y_i_pred = w@X[i]\n",
        "        s = s + (y_i_true - y_i_pred)**2\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LinearRegression coefficients\n",
        "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eqGirR9rCAm",
        "colab_type": "text"
      },
      "source": [
        "## Classification loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VskqKx2rrCVt",
        "colab_type": "text"
      },
      "source": [
        "B. (2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldf9FAFirSIt",
        "colab_type": "text"
      },
      "source": [
        "## Comparing the logistic and hinge losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9KTG_dwrUkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mathematical functions for logistic and hinge losses\n",
        "def log_loss(raw_model_output):\n",
        "   return np.log(1+np.exp(-raw_model_output))\n",
        "def hinge_loss(raw_model_output):\n",
        "   return np.maximum(0,1-raw_model_output)\n",
        "\n",
        "# Create a grid of values and plot\n",
        "grid = np.linspace(-2,2,1000)\n",
        "plt.plot(grid, log_loss(grid), label='logistic')\n",
        "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTC6Ho4xrtpX",
        "colab_type": "text"
      },
      "source": [
        "## Implementing logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFtqvdpBruZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The logistic loss, summed over training examples\n",
        "def my_loss(w):\n",
        "    s = 0\n",
        "    for i in range(y.size):\n",
        "        raw_model_output = w@X[i]\n",
        "        s = s + log_loss(raw_model_output * y[i])\n",
        "    return s\n",
        "\n",
        "# Returns the w that makes my_loss(w) smallest\n",
        "w_fit = minimize(my_loss, X[0]).x\n",
        "print(w_fit)\n",
        "\n",
        "# Compare with scikit-learn's LogisticRegression\n",
        "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
        "print(lr.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}