{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtmvERTyAYGJ",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXARkH-Z5K9G",
        "colab_type": "text"
      },
      "source": [
        "## Removing columns and rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TptRRl_WAWeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the 'flight' column\n",
        "flights_drop_column = flights.drop('flight')\n",
        "\n",
        "# Number of records with missing 'delay' values\n",
        "flights_drop_column.filter('delay IS NULL').count()\n",
        "\n",
        "# Remove records with missing 'delay' values\n",
        "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
        "\n",
        "# Remove records with missing values in any column and get the number of remaining rows\n",
        "flights_none_missing = flights_valid_delay.dropna()\n",
        "print(flights_none_missing.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bat9M0rfBKrw",
        "colab_type": "text"
      },
      "source": [
        "## Column manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYD7NiJhBK9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the required function\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "# Convert 'mile' to 'km' and drop 'mile' column\n",
        "flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
        "                    .drop('mile')\n",
        "\n",
        "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
        "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
        "\n",
        "# Check first five records\n",
        "flights_km.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA8_14RZCAvs",
        "colab_type": "text"
      },
      "source": [
        "## Categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1EjNh6kCBQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Create an indexer\n",
        "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
        "\n",
        "# Indexer identifies categories in the data\n",
        "indexer_model = indexer.fit(flights)\n",
        "\n",
        "# Indexer creates a new column with numeric index values\n",
        "flights_indexed = indexer_model.transform(flights)\n",
        "\n",
        "# Repeat the process for the other categorical feature\n",
        "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn8wLhCkCr5z",
        "colab_type": "text"
      },
      "source": [
        "## Assembling columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_0TzqxfCsdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary class\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Create an assembler object\n",
        "assembler = VectorAssembler(inputCols=[\n",
        "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
        "], outputCol='features')\n",
        "\n",
        "# Consolidate predictor columns\n",
        "flights_assembled = assembler.transform(flights)\n",
        "\n",
        "# Check the resulting column\n",
        "flights_assembled.select('features', 'delay').show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO-WBsJtCyfi",
        "colab_type": "text"
      },
      "source": [
        "## Train/test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et24-1AcCzCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into training and testing sets in a 80:20 ratio\n",
        "flights_train, flights_test = flights.randomSplit([0.8, 0.2], seed=17)\n",
        "\n",
        "# Check that training set has around 80% of records\n",
        "training_ratio = flights_train.count() / flights.count()\n",
        "print(training_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICSaodqPC4E0",
        "colab_type": "text"
      },
      "source": [
        "## Build a Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZLqQzjDC422",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Decision Tree Classifier class\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# Create a classifier object and fit to the training data\n",
        "tree = DecisionTreeClassifier()\n",
        "tree_model = tree.fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data and take a look at the predictions\n",
        "prediction = tree_model.transform(flights_test)\n",
        "prediction.select('label', 'prediction', 'probability').show(5, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyrMTKWuC9vD",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTNyO-t3DAYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a confusion matrix\n",
        "prediction.groupBy('label', 'prediction').count().show()\n",
        "\n",
        "# Calculate the elements of the confusion matrix\n",
        "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
        "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
        "\n",
        "# Accuracy measures the proportion of correct predictions\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tc6IyE-DGkS",
        "colab_type": "text"
      },
      "source": [
        "## Build a Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWhvaftODHFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the logistic regression class\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Create a classifier object and train on training data\n",
        "logistic = LogisticRegression().fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data and show confusion matrix\n",
        "prediction = logistic.transform(flights_test)\n",
        "prediction.groupBy('label', 'prediction').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A-6hStRDg-Z",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WFNQK_ADhdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
        "\n",
        "# Find weighted precision\n",
        "multi_evaluator = MulticlassClassificationEvaluator()\n",
        "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "\n",
        "# Find AUC\n",
        "binary_evaluator = BinaryClassificationEvaluator()\n",
        "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOdVOf6GDtho",
        "colab_type": "text"
      },
      "source": [
        "## Punctuation, numbers and tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6TQYDrnDtys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary functions\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "# Remove punctuation (REGEX provided) and numbers\n",
        "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
        "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
        "\n",
        "# Merge multiple spaces\n",
        "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
        "\n",
        "# Split the text into words\n",
        "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
        "\n",
        "wrangled.show(4, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yqmKkJcD1dY",
        "colab_type": "text"
      },
      "source": [
        "## Stop words and hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uebdN3AD2BX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "# Remove stop words.\n",
        "wrangled = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
        "      .transform(sms)\n",
        "\n",
        "# Apply the hashing trick\n",
        "wrangled = HashingTF(inputCol='terms', outputCol='hash', numFeatures=1024)\\\n",
        "      .transform(wrangled)\n",
        "\n",
        "# Convert hashed symbols to TF-IDF\n",
        "tf_idf = IDF(inputCol='hash', outputCol='features')\\\n",
        "      .fit(wrangled).transform(wrangled)\n",
        "      \n",
        "tf_idf.select('terms', 'features').show(4, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S8IBrExEIOb",
        "colab_type": "text"
      },
      "source": [
        "## Training a spam classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ojb9RcnEIvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into training and testing sets\n",
        "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\n",
        "\n",
        "# Fit a Logistic Regression model to the training data\n",
        "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "prediction = logistic.transform(sms_test)\n",
        "\n",
        "# Create a confusion matrix, comparing predictions to known labels\n",
        "prediction.groupBy('label', 'prediction').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}