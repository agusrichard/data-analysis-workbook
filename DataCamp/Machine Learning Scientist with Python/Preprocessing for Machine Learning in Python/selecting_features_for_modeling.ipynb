{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "selecting_features_for_modeling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RSsrLPCg0h",
        "colab_type": "text"
      },
      "source": [
        "# Selecting features for modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0fqqNzwCh75",
        "colab_type": "text"
      },
      "source": [
        "## When to use feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MZbSjp4DFUT",
        "colab_type": "text"
      },
      "source": [
        "B. A text field that hasn't been turned into a tf/idf vector yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4qz0PzdDfeJ",
        "colab_type": "text"
      },
      "source": [
        "## Identifying areas for feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXiRzsPBDiSZ",
        "colab_type": "text"
      },
      "source": [
        "D. All of the above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxCCDKL7D7ub",
        "colab_type": "text"
      },
      "source": [
        "## Selecting relevant features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW4z0YXcEmCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a list of redundant column names to drop\n",
        "to_drop = [\"locality\", \"region\", \"category_desc\", \"created_date\", \"vol_requests\"]\n",
        "\n",
        "# Drop those columns from the dataset\n",
        "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
        "\n",
        "# Print out the head of the new dataset\n",
        "print(volunteer_subset.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTimEmYYFSCI",
        "colab_type": "text"
      },
      "source": [
        "## Checking for correlated features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-ydGQjyFSYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print out the column correlations of the wine dataset\n",
        "print(wine.corr())\n",
        "\n",
        "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\n",
        "to_drop = \"Flavanoids\"\n",
        "\n",
        "# Drop that column from the DataFrame\n",
        "wine = wine.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QPBHE5V8kN",
        "colab_type": "text"
      },
      "source": [
        "## Exploring text vectors, part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HK0-DNTV9El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add in the rest of the parameters\n",
        "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
        "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
        "    \n",
        "    # Let's transform that zipped dict into a series\n",
        "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
        "    \n",
        "    # Let's sort the series to pull out the top n weighted words\n",
        "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
        "    return [original_vocab[i] for i in zipped_index]\n",
        "\n",
        "# Print out the weighted words\n",
        "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "874D9i76WtCO",
        "colab_type": "text"
      },
      "source": [
        "## Exploring text vectors, part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_hgBgOfWtbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
        "    filter_list = []\n",
        "    for i in range(0, vector.shape[0]):\n",
        "    \n",
        "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
        "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
        "        filter_list.extend(filtered)\n",
        "    # Return the list in a set, so we don't get duplicate word indices\n",
        "    return set(filter_list)\n",
        "\n",
        "# Call the function to get the list of word indices\n",
        "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
        "\n",
        "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
        "filtered_text = text_tfidf[:, list(filtered_words)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7zGZMUHXIvJ",
        "colab_type": "text"
      },
      "source": [
        "## Training Naive Bayes with feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn6l4NeLXJGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the dataset according to the class distribution of category_desc, using the filtered_text vector\n",
        "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
        "\n",
        "# Fit the model to the training data\n",
        "nb.fit(train_X, train_y)\n",
        "\n",
        "# Print out the model's accuracy\n",
        "print(nb.score(test_X, test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmK5nxVRYC_r",
        "colab_type": "text"
      },
      "source": [
        "## Using PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YebpOeDDYDgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Set up PCA and the X vector for diminsionality reduction\n",
        "pca = PCA()\n",
        "wine_X = wine.drop(\"Type\", axis=1)\n",
        "\n",
        "# Apply PCA to the wine dataset X vector\n",
        "transformed_X = pca.fit_transform(wine_X)\n",
        "\n",
        "# Look at the percentage of variance explained by the different components\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5OM1Su6YO3l",
        "colab_type": "text"
      },
      "source": [
        "## Training a model with PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG38qf4UYPP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the transformed X and the y labels into training and test sets\n",
        "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, y)\n",
        "\n",
        "# Fit knn to the training data\n",
        "knn.fit(X_wine_train, y_wine_train)\n",
        "\n",
        "# Score knn on the test data and print it out\n",
        "print(knn.score(X_wine_test, y_wine_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}