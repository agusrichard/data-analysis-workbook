{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_selection2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhHzRlsgMcWN",
        "colab_type": "text"
      },
      "source": [
        "# Feature selection II, selecting for model accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBXMl7pMihL",
        "colab_type": "text"
      },
      "source": [
        "## Building a diabetes classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T4jdCPmMone",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the scaler on the training features and transform these in one go\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "\n",
        "# Fit the logistic regression model on the scaled training data\n",
        "lr.fit(X_train_std, y_train)\n",
        "\n",
        "# Scale the test features\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Predict diabetes presence on the scaled test set\n",
        "y_pred = lr.predict(X_test_std)\n",
        "\n",
        "# Prints accuracy metrics and feature coefficients\n",
        "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n",
        "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DZbqm0INDLn",
        "colab_type": "text"
      },
      "source": [
        "## Manual Recursive Feature Elimination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0Ox16SgNDoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the feature with the lowest model coefficient\n",
        "X = diabetes_df[['pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
        "\n",
        "# Performs a 25-75% train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Scales features and fits the logistic regression model\n",
        "lr.fit(scaler.fit_transform(X_train), y_train)\n",
        "\n",
        "# Calculates the accuracy on the test set and prints coefficients\n",
        "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
        "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
        "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MYLPC_MNZJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the 2 features with the lowest model coefficients\n",
        "X = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]\n",
        "\n",
        "# Performs a 25-75% train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Scales features and fits the logistic regression model\n",
        "lr.fit(scaler.fit_transform(X_train), y_train)\n",
        "\n",
        "# Calculates the accuracy on the test set and prints coefficients\n",
        "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
        "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
        "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmQhq-NhNe-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only keep the feature with the highest coefficient\n",
        "X = diabetes_df[['glucose']]\n",
        "\n",
        "# Performs a 25-75% train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Scales features and fits the logistic regression model to the data\n",
        "lr.fit(scaler.fit_transform(X_train), y_train)\n",
        "\n",
        "# Calculates the accuracy on the test set and prints coefficients\n",
        "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
        "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
        "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-TPuEHeNhsG",
        "colab_type": "text"
      },
      "source": [
        "## Automatic Recursive Feature Elimination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq13kn5sNiD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
        "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
        "\n",
        "# Fits the eliminator to the data\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Print the features and their ranking (high = dropped early on)\n",
        "print(dict(zip(X.columns, rfe.ranking_)))\n",
        "\n",
        "# Print the features that are not eliminated\n",
        "print(X.columns[rfe.support_])\n",
        "\n",
        "# Calculates the test set accuracy\n",
        "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
        "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exvWPrJDOibC",
        "colab_type": "text"
      },
      "source": [
        "## Building a random forest model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eSNjlOjOiy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform a 75% training and 25% test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Fit the random forest model to the training data\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "\n",
        "# Print the importances per feature\n",
        "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
        "\n",
        "# Print accuracy\n",
        "print(\"{0:.1%} accuracy on test set.\".format(acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuRuHI8SO_Sh",
        "colab_type": "text"
      },
      "source": [
        "## Random forest for feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OEIMEPtO_qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a mask for features importances above the threshold\n",
        "mask = rf.feature_importances_ > 0.15\n",
        "\n",
        "# Prints out the mask\n",
        "print(mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvUTvBvpPMKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a mask for features importances above the threshold\n",
        "mask = rf.feature_importances_ > 0.15\n",
        "\n",
        "# Apply the mask to the feature dataset X\n",
        "reduced_X = X.loc[:, mask]\n",
        "\n",
        "# prints out the selected column names\n",
        "print(reduced_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GptUK9SjPQK5",
        "colab_type": "text"
      },
      "source": [
        "## Recursive Feature Elimination with random forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfK1cj_BPQjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrap the feature eliminator around the random forest model\n",
        "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1oeg21LPgQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrap the feature eliminator around the random forest model\n",
        "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rfe.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpT_7r3SPoxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrap the feature eliminator around the random forest model\n",
        "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Create a mask using an attribute of rfe\n",
        "mask = rfe.support_\n",
        "\n",
        "# Apply the mask to the feature dataset X and print the result\n",
        "reduced_X = X.loc[:, mask]\n",
        "print(reduced_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DKwCe_bPsu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the feature eliminator to remove 2 features on each step\n",
        "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Create a mask\n",
        "mask = rfe.support_\n",
        "\n",
        "# Apply the mask to the feature dataset X and print the result\n",
        "reduced_X = X.loc[:, mask]\n",
        "print(reduced_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gTZEnQ0QtJd",
        "colab_type": "text"
      },
      "source": [
        "## Creating a LASSO regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKqvXyHUQtjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the test size to 30% to get a 70-30% train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Fit the scaler on the training features and transform these in one go\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "\n",
        "# Create the Lasso model\n",
        "la = Lasso()\n",
        "\n",
        "# Fit it to the standardized training data\n",
        "la.fit(X_train_std, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVfhP9mVQ_l-",
        "colab_type": "text"
      },
      "source": [
        "## Lasso model results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJUAqxwyQ_9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform the test set with the pre-fitted scaler\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Calculate the coefficient of determination (R squared) on X_test_std\n",
        "r_squared = la.score(X_test_std, y_test)\n",
        "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
        "\n",
        "# Create a list that has True values when coefficients equal 0\n",
        "zero_coef = la.coef_ == 0\n",
        "\n",
        "# Calculate how many features have a zero coefficient\n",
        "n_ignored = sum(zero_coef)\n",
        "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRMsO_KhRTnF",
        "colab_type": "text"
      },
      "source": [
        "## Adjusting the regularization strength"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHSTnYPeRUAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the highest alpha value with R-squared above 98%\n",
        "la = Lasso(alpha=0.1, random_state=0)\n",
        "\n",
        "# Fits the model and calculates performance stats\n",
        "la.fit(X_train_std, y_train)\n",
        "r_squared = la.score(X_test_std, y_test)\n",
        "n_ignored_features = sum(la.coef_ == 0)\n",
        "\n",
        "# Print peformance stats \n",
        "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
        "print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW9z045ySZxR",
        "colab_type": "text"
      },
      "source": [
        "## Creating a LassoCV regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WOtvUfKSaKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "# Create and fit the LassoCV model on the training set\n",
        "lcv = LassoCV()\n",
        "lcv.fit(X_train, y_train)\n",
        "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
        "\n",
        "# Calculate R squared on the test set\n",
        "r_squared = lcv.score(X_test, y_test)\n",
        "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
        "\n",
        "# Create a mask for coefficients not equal to zero\n",
        "lcv_mask = lcv.coef_ != 0\n",
        "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhQAHBSLS5TL",
        "colab_type": "text"
      },
      "source": [
        "## Ensemble models for extra votes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkW030r3S5qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
        "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
        "             n_features_to_select=10, step=3, verbose=1)\n",
        "rfe_gb.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6GEUFIZTH2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
        "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
        "             n_features_to_select=10, step=3, verbose=1)\n",
        "rfe_gb.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the R squared on the test set\n",
        "r_squared = rfe_gb.score(X_test, y_test)\n",
        "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1J-fGirTRUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
        "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
        "             n_features_to_select=10, step=3, verbose=1)\n",
        "rfe_gb.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the R squared on the test set\n",
        "r_squared = rfe_gb.score(X_test, y_test)\n",
        "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
        "\n",
        "# Assign the support array to gb_mask\n",
        "gb_mask = rfe_gb.support_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uPM-KQzTVAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
        "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
        "             n_features_to_select=10, step=3, verbose=1)\n",
        "rfe_rf.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the R squared on the test set\n",
        "r_squared = rfe_rf.score(X_test, y_test)\n",
        "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
        "\n",
        "# Assign the support array to gb_mask\n",
        "rf_mask = rfe_rf.support_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPnEP7EDTYYZ",
        "colab_type": "text"
      },
      "source": [
        "## Combining 3 feature selectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eElnY-hUTYwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sum the votes of the three models\n",
        "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
        "print(votes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V9yFSfWTwaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sum the votes of the three models\n",
        "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
        "print(votes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJrW19ovT0JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sum the votes of the three models\n",
        "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
        "\n",
        "# Create a mask for features selected by all 3 models\n",
        "meta_mask = votes >= 3\n",
        "\n",
        "# Apply the dimensionality reduction on X\n",
        "X_reduced = X.loc[:, meta_mask]\n",
        "print(X_reduced.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1fmAYFqT39Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sum the votes of the three models\n",
        "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
        "\n",
        "# Create a mask for features selected by all 3 models\n",
        "meta_mask = votes >= 3\n",
        "\n",
        "# Apply the dimensionality reduction on X\n",
        "X_reduced = X.loc[:, meta_mask]\n",
        "\n",
        "# Plug the reduced dataset into a linear regression pipeline\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
        "lm.fit(scaler.fit_transform(X_train), y_train)\n",
        "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
        "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}